%allgemeine todos
\todo{drbd ausformulieren -> christian}
\todo{einleitung -> sebastian}
\todo{fazit -> sebastian}
\todo{higlights + todos -> michael}


\chapter{Einleitung}
\todo{Neuschreiben, Wheezy erwähnen, Zielsetzung}
Notwendige Anpassungen des Hosts, damit wir VMs erstellen können...
Unser Praktikum unterteilt sich neben der Erstellung von virtuellen Maschinen (VMs) auf fünf größere Aufgaben (kapitel? bereiche?)... \\
Die lokale Infrastruktur auf unserem Rechner ergänzen wir mit einem \emph{Logical Volume Manager} (LVM), aufbauend auf einem \emph{Redundant Array of Independent Disks} (RAID) zur Speicherverwaltung und Open vSwitch als Ersatz der standardmäßigen Bridge. \\
Erstellt haben wir insgesamt vier virtuelle Maschinen (VMs) mit unterschiedlichen Parametern und Einstellungen. Dabei testeten wir verschiedene Netzschnittstellentypen und verschiedene Virtualisierungsmethoden. \\
Für die Live-Migration der VMs verwendeten wir Blockdevices, die über verschiedene Möglichkeiten in der Praktikumsumgebung zwischen den Hosts freigegeben wurden. Dazu benutzten wir \emph{ATA-Over-Ethernet} (AOE), wo die Daten nur auf einem der Hosts hinterlegt sind, und \emph{Distributed Replicated Block Device} (DRBD), bei dem die Daten redundant auf mehreren Hosts gespeichert werden. 
\chapter{Vorarbeit}
\label{chap:whezzy_ugprade}
Bevor mit dem eigentlichen Praktikum begonnen wurde, haben wir einige Vorarbeiten erledigt: \\ 
Um aktuellere Versionen von Xen einsetzen zu können, haben wir mit einem Distributionsupgrade des Praktikumssystems von Debian Squeeze auf Wheezy begonnen. \\ 
Dazu wird in \verb#/etc/apt/sources.list# jedes Vorkommen von \textquotedblleft\verb#squeeze#\textquotedblright ~ durch \textquotedblleft\verb#wheezy#\textquotedblright ~ ersetzt und folgende Befehle ausgeführt: 

\setupVerbatim{bash}
\begin{verbatim}
apt-get update
apt-get dist-upgrade -y
\end{verbatim}
Zusätzlich haben wir einen Symlink 
\setupVerbatimOut 
\begin{verbatim} 
09_linux_xen -> 20_linux_xen 
\end{verbatim} 
in \verb#/etc/grub.d# erstellt, damit der Xen Hypervisor standardmäßig geladen wird.
\\
Um die Konnektivität zum internen Netz der VMs nach dem Bootvorgang zu gewährleisten, haben wir die Konfiguration in \verb#/etc/network/interfaces# um \verb#eth0# erweitert. Darüber hinaus wurde der Nameserver in \verb#/etc/resolv.conf# auskommentiert, da dieser Probleme beim Auflösen von DNS-Einträgen außerhalb der Praktikumsumgebung hatte.

Der nächste Schritt ist nun die Einrichtung eines Datenspeichers für unsere Virtuellen Maschinen.

\chapter{Storage}
Um einen Datenspeicher für spätere virtuelle Maschinen zu erhalten, entschlossen wir uns einen \emph{Redundant Array of Independent Disks} (RAID) Verbund über zwei Festplatten zu erzeugen (vgl. Abschnitt \ref{raid}). Wir entschieden uns für RAID Level 1. Dieser bietet durch die Spiegelung der Partitionen eine gewisse Daten- und Ausfallsicherheit. Als tatsächlicher Datenspeicher sollten schließlich mit dem \emph{Logical Volume Manager} (LVM) erzeugte \emph{Logical Volumes} (LV) dienen, was in Abschnitt \ref{lvm} umgesetzt wird.
\section{RAID}
\label{raid}
Für einen RAID-Verbund werden mindestens zwei Festplatten benötigt, welche möglichst identisch sein sollten. Wir verwenden die Platten \verb#sdb# und \verb#sdc# auf \verb#pcvirt01#. \\
Zum Erzeugen einer \emph{GUID Partition Table} (GPT) nutzten wir \verb#gdisk#, welches vorher installiert werden musste. \\ 
Zur reinen Erzeugung eines RAID Systems ist dies nicht zwingend erforderlich, jedoch stellt GPT den designierten Nachfolgestandard für MBR-Partitionstabellen dar. \todo{satz ggf. verbessern}
\setupVerbatim{bash}
\begin{verbatim}
apt-get install gdisk
gdisk /dev/sdb
\end{verbatim}
Auf \verb#/dev/sdb# wurden daraufhin zwei Partitionen erzeugt. Die erste für GPT mit ca. 1 Megabyte. Die zweite mit dem restlichen Platz der Festplatte.
Diese Partitionstabelle wurde mit \verb#sgdisk -R# auf die 2. Platte übertragen und danach dessen GUID mit \verb#sgdisk -G# wieder randomisiert.
\setupVerbatim{bash}
\begin{verbatim}
sgdisk -R=/dev/sdc /dev/sdb
sgdisk -G /dev/sdb
\end{verbatim}
Abschließend wurde der RAID-Verbund \verb#/dev/md1# über die beiden Partitionen \verb#/dev/sdb2# und \verb#/dev/sdc2# mit \verb#mdadm# erzeugt. Der Parameter \verb#-n# bezeichnet die Anzahl der Partitionen und \verb#-l# gibt den RAID Level an.
\setupVerbatim{bash}
\begin{verbatim}
mdadm --create /dev/md1 -n 2 -l 1 /dev/sdb2 /dev/sdc2
\end{verbatim}
Die Erstellung des RAID-Verbundes ist anhand der Ausgabe von \verb#dmesg# nachvollziehbar. Zur Überprüfung, ob die Erstellung erfolgreich war, ist der Inhalt von \verb#/proc/mdstat# hilfreich: 
\setupVerbatimOut
\begin{verbatim}
$ dmesg | tail
[  253.106706] sdb: sdb1 sdb2
[  364.210743] sdc: sdc1 sdc2
[  373.659167] sdb: sdb1 sdb2
[  701.414204] md: bind<sdb2>
[  701.415194] md: bind<sdc2>
[  701.425611] md: raid1 personality registered for level 1
[  701.425853] bio: create slab <bio-1> at 1
[  701.425897] md/raid1:md1: not clean -- starting background reconstruction
[  701.425898] md/raid1:md1: active with 2 out of 2 mirrors
[  701.425913] md1: detected capacity change from 0 to 499971325952
[  701.433300] md1: unknown partition table
[ 1223.952091] md: resync of RAID array md1
[ 1223.952093] md: minimum _guaranteed_  speed: 1000 KB/sec/disk.
[ 1223.952095] md: using maximum available idle IO bandwidth (but not more than 200000 KB/sec) for resync.
[ 1223.952097] md: using 128k window, over a total of 488253248k.

$ cat /proc/mdstat
Personalities : [raid1] 
md1 : active raid1 sdc2[1] sdb2[0]
      488253248 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>
\end{verbatim}

Aus der Zeile \verb#md1 : active raid1 sdc2[1] sdb2[0]# lässt sich ablesen, dass das RAID-Device verfügbar ist. \verb#[UU]# zeigt an, dass beide gespiegelte Partitionen aktuell (Up) sind \cite{wiki_mdstat}.

\section{LVM}
\label{lvm}
Aufbauend auf dem RAID-Device haben wir ein LVM-System angelegt. Dazu wurde der soeben generierte RAID1-Verbund \verb#/dev/md1# aus Abschnitt \ref{raid} mit \verb#pvcreate# als Physical Volume (PV) initialisiert und mit \verb#vgcreate# in eine neue Volume Group \verb#storage# eingebunden. 
Dann haben wir mit \verb#lvcreate# zwei LVs in dieser Gruppe erstellt. Dabei gibt der Parameter \verb#-n# den Namen des LVs an, \verb#-L# die Größe. 
\setupVerbatim{bash}
\begin{verbatim}
pvcreate /dev/md1 
vgcreate storage /dev/md1
lvcreate -n guest1 -L 10G storage
lvcreate -n guest2 -L 20G storage
\end{verbatim} 
Mit dem Befehl \verb#lvm# kann der Status der Konfiguration überprüft werden. Dabei zeigt der Kommando \verb#lvs# Informationen der Logical Volumes und \verb#pvs# Informationen der Physical Volumes an. 
\setupVerbatimOut
\setupVerbatim{bash}
\begin{verbatim}
lvm> lvs
  LV               VG      Attr     LSize   Pool Origin Data%  Move Log Copy%  Convert                                        
  guest1           storage -wi-ao--  10.00g                                           
  guest2           storage -wi-ao--  20.00g                                           
lvm> pvs
  PV         VG      Fmt  Attr PSize   PFree  
  /dev/md1   storage lvm2 a--  465.63g 436.13g
\end{verbatim} 
Beim Erstellen der VMs haben wir festgestellt, dass dabei neue Partitionen für die Gastbetriebssysteme angelegt werden (\verb#guestX-disk# und \verb#guestX-swap#). Somit ist es also nicht notwendig diese von Hand zu erstellen. 

\chapter{Open vSwitch}
\label{chap:ovs}
Da wir die von Linux mitgelieferte Bridge-Implementierung durch Open vSwitch (OVS) ersetzen wollten, waren einige weitere Schritte notwendig. Die Vorteile, die Open vSwitch im Bereich VLAN-Trunking bringt, welches in Kapitel \ref{vlan_trunk} eingesetzt werden soll, wiegen diese Zusatzarbeiten unserer damaligen Einschätzung nach auf.
\\
Die Installation gestaltet sich auf dem Debian Wheezy System relativ einfach, da mit \verb#apt-get# die Pakete problemlos geladen werden können:
\setupVerbatim{bash}
\begin{verbatim}
apt-get install openvswitch-brcompat openvswitch-switch openvswitch-datapath-dkms
\end{verbatim}

Ein anschließendes \verb#modprobe openvswitch-mod# lädt das Kernelmodul von Open vSwitch. Dessen erfolgreiche Installation kann man der Ausgabe von \verb#dmesg# entnehmen:
\setupVerbatimOut
\begin{verbatim}
[ 2377.104677] openvswitch_mod: Open vSwitch switching datapath 1.4.2, built Apr 30 2013 15:47:50
\end{verbatim}
Das Paket \verb#openvswitch-brcompat# bietet eine Unterstützung für Programme, die bislang das Kommando \verb#brctl# des \verb#bridge# Moduls nutzten, um mit Open vSwitch weiterhin zu funktionieren. Damit diese Unterstützung aktiviert wird, musste in der Datei \verb#/etc/default/openvswitch-switch# der Schalter \verb#BRCOMPAT=yes# gesetzt werden. Anschließend wurde der OVS-Dienst gestartet und eine Bridge für die VMs erzeugt:
\setupVerbatimBash
\begin{verbatim}
/etc/init.d/openvswitch-switch start
ovs-vsctl add-br br-guest
\end{verbatim}
Die erfolgreiche Erstellung kann mittels \verb#ovs-vsctl show# überprüft werden:
\setupVerbatimOut
\begin{verbatim}
b01c6804-1ad5-4294-98d1-b6aa0a8f6155
    Bridge br-guest
        Port br-guest
            Interface br-guest
                type: internal
    ovs_version: "1.4.2"
\end{verbatim} 
Um eine Verbindung der Bridge in die physische Welt zu schaffen, musste eine physische Schnittstelle zur Bridge hinzugefügt werden.
\setupVerbatimBash
\begin{verbatim}
ovs-vsctl add-port br-guest eth0
\end{verbatim}
Das Ergebnis wurde abermals mit \verb#ovs-vsctl show# überprüft:
\setupVerbatimOut
\begin{verbatim}
b01c6804-1ad5-4294-98d1-b6aa0a8f6155
     Bridge br-guest
        Port "eth0"
            Interface "eth0"
        Port br-guest
            Interface br-guest
                type: internal
    ovs_version: "1.4.2"
\end{verbatim}

Anschließend wurde die Konfiguration der Schnittstellen in \verb#/etc/network/interfaces# angepasst, damit anstatt dem einzelnen Interface \verb#eth0# beim Start des Systems die Bridge \verb#br-guest# konfiguriert wird:
\setupVerbatimOut
\begin{verbatim}
auto lo eth1

# The loopback network interface
iface lo inet loopback

# The primary network interface
iface eth1 inet static
    address 10.163.235.131
    netmask 255.255.255.128
    gateway 10.163.235.254

auto br-guest
iface br-guest inet static
    address 192.168.1.131
    netmask 255.255.255.0
    bridge_ports eth0
\end{verbatim}

Die oben getätigten Einstellungen des \verb#ovs-vsctl# Kommandos werden ohne weiteres Zutun durch Open vSwitch persistiert. Dies stellt einen Vorteil im vergleich zur klassischen Linux Bridge dar. Dies alleine rechtfertigt schon den Aufwand zur Installation von Open vSwitch. 
\\
Anschließend wurde die Konfiguration, durch einen Neustart geladen. Die Konnektivität zu anderen Hosts im internen Praktikumsnetz wurde mittels \verb#ping# erfolgreich überprüft.
\\
\\
Die grundlegenden Anforderungen an die Infrastruktur des Hostrechners sind damit erfüllt, sodass im nächsten Kapitel die erste VM erstellt werden kann.

\chapter{Virtuelle Maschinen}
Nachdem nun alle notwendigen Vorbereitungen abgeschlossen waren, konnte die erste \emph{virtuelle Maschine} (VM) erstellt werden. Dazu wurde das Kommando \verb#xen-create-image# genutzt, welches die Erstellung einer paravirtualisierten VM weitestgehend automatisiert. Es wird zuerst die VM \verb#guest1# mit dem Betriebssystem Debian Wheezy, anschließend dann \verb#guest2# mit Debian Squeezy erstellt.
\\
Im Abschnitt \ref{hvm_vm} wird dann eine vollvirtualisierte VM \verb#guest4# wiederum mit Debian Wheezy erstellt. Bei dieser Vollvirtualisierung läuft das Gastbetriebssystem prinzipiell ohne Anpassungen an den Hypervisor. Um IO-Operationen zu beschleunigen werden jedoch spezielle Treiber für Netzschnittstellen und Festplatten verwendet. 
\\
Die Performanz der verschiedenen Virtualisierungsmethoden bei Netzschnittstellen wird im letzten Abschnitt des Kapitels (Abschnitt \ref{perf}) anhand eines Experimentes untersucht.

\section{Erstellung einer paravirtualisierten VM}
Um die ersten paravirtualisierten VMs auf dem Host \verb#pcvirt01# zu erzeugen, wurde das Skript \verb#xen-create-image# genutzt. Diesem kann man über diverse Parameter Einstellungen übergeben. Des weiteren übernimmt es die Installation der paravirtualisierten Gastbetriebssysteme für den Nutzer.
\setupVerbatim{bash}
\begin{verbatim}
xen-create-image --ip 192.168.10.12  --lvm=storage --hostname=guest1 --vcpus=2 --dist wheezy
\end{verbatim}
\setupVerbatimOut
\begin{verbatim}                                                  
WARNING                                           
-------                                           
                                                  
  You appear to have a missing vif-script, or network-script, in the
 Xen configuration file /etc/xen/xend-config.sxp. 
                                                  
  Please fix this and restart Xend, or your guests will not be able
 to use any networking!                           
                                                  
WARNING:  No gateway address specified!           
WARNING:  No netmask address specified!           
                                                  
General Information                               
--------------------                              
Hostname       :  guest1                          
Distribution   :  wheezy                          
Mirror         :  http://cdn.debian.net/debian/   
Partitions     :  swap            128Mb (swap)    
                  /               4Gb   (ext3)    
Image type     :  full                            
Memory size    :  128Mb                           
Kernel path    :  /boot/vmlinuz-3.2.0-4-amd64     
Initrd path    :  /boot/initrd.img-3.2.0-4-amd64  
                                                  
Networking Information                            
----------------------                            
IP Address 1   : 192.168.10.12 [MAC: 00:16:3E:95:61:15]
                                                  
                                                  
Creating swap on /dev/storage/guest1-swap         
Done                                              
                                                  
Creating ext3 filesystem on /dev/storage/guest1-disk
Done                                              
Installation method: debootstrap
Done

Running hooks
Done

No role scripts were specified.  Skipping

Creating Xen configuration file
Done

No role scripts were specified.  Skipping
Setting up root password
Generating a password for the new guest.
All done


Logfile produced at:
         /var/log/xen-tools/guest1.log

Installation Summary
---------------------
Hostname        :  guest1
Distribution    :  wheezy
IP-Address(es)  :  192.168.10.12 
RSA Fingerprint :  01:06:31:35:f0:d4:f0:70:54:d3:0f:f2:d4:90:ba:e1
Root Password   :  JzXi4Ufg
\end{verbatim}

Anschließend haben wir die Konfiguration \verb#/etc/xen/guest1.cfg# der VM noch von Hand angepasst um die Anzahl der CPUs zu verändern und mehr Arbeitsspeicher zu erhalten:
\setupVerbatimOut
\begin{verbatim}
    vcpus       = '3'
    memory      = '512'
\end{verbatim}

Abschließend wurde die VM gestartet:
\setupVerbatim{bash}
\begin{verbatim}
xm create /etc/xen/guest1.cfg
\end{verbatim}

Auf dem Host wurde hierdurch eine Schnittstelle \verb#vif1.0# erstellt und mit der Bridge \verb#br-guest# verbunden. Nach dem Wechsel in die Konsole der VM mittels \verb#xm# \verb#console# \verb#guest1# wurde die Konnektivität zum Host mittels \verb#ping# auf der VM erfolgreich überprüft. Zum Verlassen der Konsole dient \verb#Ctrl+AltGr+9#.

\section{Weitere VM und Bridge für VM-Interconnect}
Der nächste Versuch erweiterte den aktuellen Aufbau um eine weitere VM und eine zusätzliche Bridge. Diese verbindet ausschließlich Schnittstellen der VMs und besitzt keine Anbindung an ein physisches Interface. Später soll diese Bridge über ein VLAN auch VMs auf zwei verschiedenen Hosts verbinden.
\setupVerbatim{bash}
\begin{verbatim}
xen-create-image --ip 192.168.10.13  --lvm=storage --hostname=guest2 --vcpus=2 --bridge=br-guest
ovs-vsctl add-br br-vmonly
\end{verbatim}
Um ein weiteres Interface auf der Bridge br-vmonly zu erhalten wurde die soeben erstellte Konfiguration \verb#/etc/xen/guest2.cfg# angepasst:
\setupVerbatimOut
\begin{verbatim}
vif         = [
    'mac=00:16:3E:32:D0:3E,bridge=br-guest',
    'mac=00:16:3E:32:CA:FE,bridge=br-vmonly' 
             ]
\end{verbatim}
Auf \verb#guest1# geschah das Hinzufügen des 2. Interfaces analog.\\
Anschließend wurden die VMs gestartet und die Datentransferrate zwischen den VMs mittels \verb#iperf# bestimmt. Die gemessenen Übertragungsraten in der Größenordnung von 8 $\frac{Gbit}{s}$ und sind damit deutlich höher als bei Vergleichsmessungen zwischen einer VM und einem anderen physischen System.

\setupVerbatimOut
\begin{verbatim}
------------------------------------------------------------
Client connecting to 192.168.2.2, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 192.168.2.1 port 57804 connected with 192.168.2.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  9.21 GBytes  @\bfseries 7.91@ @\bfseries Gbits/sec@


------------------------------------------------------------
Client connecting to 192.168.1.2, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 192.168.1.1 port 56132 connected with 192.168.1.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  10.6 GBytes  @\bfseries 9.12@ @\bfseries Gbits/sec@
\end{verbatim}
Ein nebenbei laufender \verb#tcpdump# auf dem empfangenden System zeigte sehr große eingehende Ethernet-Frames mit Größen bis zu 65226 Byte: 
\setupVerbatimOut
\begin{verbatim}
17:45:48.414535 00:16:3e:32:d0:3e > 00:16:3e:95:de:ad, ethertype IPv4 (0x0800), @\bfseries length@ @\bfseries 65226@: (tos 0x0, ttl 64, id 25535, offset 0, flags [DF], proto TCP (6), @\bfseries length@ @\bfseries 65212@)
    192.168.1.2.57025 > 192.168.1.1.5001: Flags [.], seq 14811337:14876497, ack 0, win 913, options [nop,nop,TS val 396548 ecr 356977], @\bfseries length@ @\bfseries 65160@
17:45:48.414540 00:16:3e:32:d0:3e > 00:16:3e:95:de:ad, ethertype IPv4 (0x0800), @\bfseries length@ @\bfseries 65226@: (tos 0x0, ttl 64, id 25580, offset 0, flags [DF], proto TCP (6), @\bfseries length@ @\bfseries 65212@)
    192.168.1.2.57025 > 192.168.1.1.5001: Flags [P.], seq 14876497:14941657, ack 0, win 913, options [nop,nop,TS val 396548 ecr 356977], @\bfseries length@ @\bfseries 65160@
17:45:48.414542 00:16:3e:32:d0:3e > 00:16:3e:95:de:ad, ethertype IPv4 (0x0800), @\bfseries length@ @\bfseries 65226@: (tos 0x0, ttl 64, id 25625, offset 0, flags [DF], proto TCP (6), @\bfseries length@ @\bfseries 65212@)
    192.168.1.2.57025 > 192.168.1.1.5001: Flags [.], seq 14941657:15006817, ack 0, win 913, options [nop,nop,TS val 396548 ecr 356977], @\bfseries length@ @\bfseries 65160@
17:45:48.414559 00:16:3e:95:de:ad > 00:16:3e:32:d0:3e, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 64, id 54265, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.1.1.5001 > 192.168.1.2.57025: Flags [.], cksum 0x837a (@\bfseries incorrect@ -> 0x86f4), seq 0, ack 14876497, win 16397, options [nop,nop,TS val 356977 ecr 396548], length 0
\end{verbatim}
Wohingegen Ping-Pakete, welche die MTU der \verb#vif#-Schnittstellen oder Bridges überschritten fragmentiert -- oder bei gesetztem \verb#DF#-Flag abgewiesen -- wurden.
Dies liegt daran, dass die Treiber der virtualisierten Schnittstellen dem Kernel mitteilen sie würden \emph{TCP Segment Offloading} (TSO) unterstützen. Bei dieser Technik übernimmt die Schnittstelle die Segmentierung der TCP-Daten. Im Fall der Virtualisierung werden die unsegmentierten TCP-Daten an den Hypervisor übergeben und in diesem entweder vor der Übertragung auf physische Schnittstellen segmentiert oder wiederum unsegmentiert an andere auf diesem Host laufende Gastbetriebssysteme weitergegeben. Die \verb#cksum# \verb#incorrect# Meldung entsteht ebenso durch eine Offloadingtechni. Hierbei werden die Ethernet-Checksummen von der Schnittstelle und nicht dem OS gesetzt. 
Zusätzlich wurde getestet, ob die Bridges und \verb#vif#-Schnittstellen mit Jumbo-Frames von 9000 Byte Größe umgehen können. Nach Erhöhung der MTU auf der Bridge, den \verb#vif#-Schnittstellen und auf den Schnittstellen des Gastes konnten Jumbo-Frames genutzt werden.

\section{Hardware Virtual Machine}
\label{hvm_vm}
Neben der Paravirtualisierung unterstützt Xen auch noch Vollvirtualiserung. Hierzu wird von Xen mit Hilfe der Virtualisierungsunterstützung der CPU ein Hypervisor installiert, der die Traps abfängt und die entsprechenden Hypercalls am eigentlichen Xen-Hypervisor aufruft.

Um eine vollvirtualisierte VM zu erstellen, muss die Konfiguration der VM angepasst werden (gekürzte Darstellung):
\setupVerbatimOut
\begin{verbatim}
kernel      = '/usr/lib/xen-4.1/boot/hvmloader'
builder     = 'hvm'
device_model= '/usr/lib/xen-4.1/bin/qemu-dm'

vnc=1
vncconsole=1
vnclisten='0.0.0.0'
vncpasswd=''
keymap='de'
\end{verbatim}

Durch die Konfiguration des Kernels zu \verb#hvmloader# und Nutzung des Builders \verb#hvm# wird eine Vollvirtualisierung angestoßen. Die VNC-Konfiguration ist notwendig, um die grafische Ausgabe der VM sehen zu können. Eine Betriebssysteminstallation kann beispielsweise durch Hinzufügen eines ISO-Images als CDROM-Laufwerk ermöglicht werden: 
\setupVerbatimOut
\begin{verbatim}
disk = [ <WEITERE KONFIGURATION> , 'file:/xen/iso/debian-7.0.0-amd64-netinst.iso,hdc:cdrom,r']
\end{verbatim}

\section{Netzperformanz unterschiedlicher VM Typen}\label{perf}

Nun wird durch ein Experiment die Netzperformanz verschiedener VM-Typen mittels \verb#iperf# zwischen Hostmaschine und VM gemessen. Es wird zwischen folgenden Schnittstellentypen unterschieden:
\begin{itemize}
\item paravirtualisierte VM (PVM)
\item vollvirtualisierte VM mit Netzschnittstellenemulation durch \verb#QEMU# (HVM)
\item vollvirtualisierte VM mit paravirtualisierten Netzschnittstellen-Treibern (HVM\_PV)
\end{itemize}

Dieser Test zeigt wie in Abbildung \ref{megaplot} ersichtlich signifikante Unterschiede bzgl. der Datenraten. Während die Messung mit HVM\_PV-Treibern -- die standardmäßig aktiviert sind -- ebenso wie die PVM hohe Datenraten zeigen, bricht die Datenrate bei der \verb#QEMU#-Emulation erheblich ein.
\\
Den Verzicht auf die HVM\_PV-Treiber konfiguriert man mit dem Parameter \verb#xen_platform_pci = 0# in der Konfigurationsdatei der VM.

Die jetzige Konfiguration des Hostsystems \verb#pcvirt01# kann somit VMs erfolgreich ausführen. Damit man nun VMs auf ein weiteres Hostsystem übertragen kann, muss auch von diesem der Hintergrundspeicher der jeweiligen VM erreichbar sein. Da dieser derzeit nur von \verb#pcvirt01# aus erreichbar ist, muss man in einem weiteren Schritt diesen über das Praktikumsnetz freigeben.
\begin{figure}
\input{plot1}
\caption{Vergleich der Netz IO Performanz}
\label{megaplot}
\end{figure}
\chapter{ATA-Over-Ethernet}
\label{chap:aoe}
Damit die VMs nicht fest an einen Host gebunden sind, soll in diesem Kapitel der Hintergrundspeicher der VM \verb#guest1# für weitere Hosts freigegeben werden. Dazu wurde \emph{ATA-Over-Ethernet} (AoE) \cite{hopkins2006aoe} eingesetzt. Eine Alternative hierzu wäre das \emph{internet Small Computer System Interface} (iSCSI) \cite{rfc3720}. Da bereits die andere Gruppe diese Technologie eingesetzt hatte, haben wir uns für AoE entschieden um eine weitere mögliche Technik zu demonstrieren. 
\\
Die anschließenden Abschnitte beschreiben die Einrichtung, Konfiguration und Inbetriebnahme von AoE auf den Hosts \verb#pcvirt01# und \verb#pcvirt02#.

\section{Einrichtung des Kernelmoduls}
Vor der Einrichtung von AoE auf den Hosts \verb#pcvirt01# und \verb#pcvirt02# haben wir in den Konfigurationen der Kernels nachgesehen, ob jeweils AoE unterstützt wird. 
\setupVerbatimOut
\begin{verbatim}
grep ATA_OVER /boot/config-`uname -r`
    CONFIG_ATA_OVER_ETH=m
\end{verbatim} 
Die Ausgabe zeigt auf beiden Rechnern die Unterstützung von AoE als Kernelmodul. Daher haben wir das Modul \verb#aoe# auf den beiden Hosts mittels \verb#modprobe aoe# geladen und  in \verb#/etc/modules# eingefügt, damit es beim Start automatisch geladen wird. 

\section{Einrichtung von vbladed}
Zum Einrichten eines AoE-Targets mittels \verb#vbladed#, muss das Blockdevice frei sein. Da die Devices von \verb#guest1# geteilt werden sollten, muss die VM gestoppt werden. Dann können mit \verb#vbladed# die Devices auf der Schnittstelle \verb#br-guest# zur Verfügung gestellt werden. Beim Starten von \verb#vbladed# müssen zur Zuordnung der freigegebenen Devices zwei Nummern (Major und Minor) vergeben werden. Da der Hintergrundspeicher auf \verb#pcvirt01# residiert, wurden diese Schritte dort durchgeführt:
\setupVerbatimOut
\begin{verbatim}
xm shutdown guest1

vbladed 0 1 br-guest /dev/storage/guest1-disk  
vbladed 0 2 br-guest /dev/storage/guest1-swap
\end{verbatim}
Damit die Devices nach einem Neustart wieder freigegeben werden, kommen die zwei letzten Befehle auch in \verb#/etc/rc.local#. 
\section{Einrichtung des Clients}
Da Gruppe 2 aus dem Praktium ausgestiegen ist und daher \verb#pcvirt02# noch nicht konfiguriert war, wurde dieser analog zu \verb#pcvirt01#  -- soweit nötig -- eingerichtet. Auf das Distributionsupgrade auf Wheezy und Open vSwitch wurde vorerst verzichtet; in Abschnitt \ref{ovs2} wird dies nötig und nachgeholt.
\setupVerbatim{bash}
\begin{verbatim}
brctl addbr br-guest
brctl addif br-guest eth0
ifconfig br-guest up
\end{verbatim}
Konfiguration für \verb#/etc/network/interfaces#:
\setupVerbatimOut
\begin{verbatim}
auto br-guest
iface br-guest inet static
        address 192.168.1.132
        netmask 255.255.255.0
        bridge_ports eth0
\end{verbatim}

Um von \verb#pcvirt02# auf die von \verb#pcvirt01# per AoE freigegebenen Blockdevices zugreifen zu können, werden diese zuerst mittels \verb#aoe-discover# erkannt, und anschließend ihr Status mittels \verb#aoe-stat# geprüft:
\setupVerbatimOut
\begin{verbatim}
aoe-discover 
aoe-stat 
      e0.1         4.294GB br-guest up 
      e0.2         0.134GB br-guest up  
\end{verbatim}

Die beiden Blockdevices sind nun als \verb#/dev/etherd/e0.X# auf dem AoE-Client \verb#pcvirt02# verfügbar.

\section{Konfiguration des Targets}\label{dirtysym}
Da die von uns genutzte AoE-Implementierung keine Targets findet, die auf demselben Host bereit gestellt werden, haben wir die Targets auf \verb#pcvirt01# mittels Symlinks an die entsprechende Position im Verzeichnisbaum gelinkt.
\setupVerbatimOut
\begin{verbatim}
ln -s /dev/storage/guest1-disk /dev/etherd/e0.1
ln -s /dev/storage/guest1-swap /dev/etherd/e0.2 
\end{verbatim}

Nötig wurde dieser Schritt, da beide Hosts mit der gleichen Konfiguration der VM arbeiten und daher jedes genutzte Blockdevice unter demselben Pfad auffindbar sein muss.

\section{Konfiguration der VM}
Die Pfade der Datenspeicher von \verb#guest1# müssen nun entsprechend den vorangegangenen Abschnitten angepasst werden, damit die VM auf beiden Hosts auf ihre Blockdevices zugreifen kann.
\\
Auf \verb#pcvirt02# wird dann AoE genutzt, auf \verb#pcvirt01# wird direkt auf das LV zugegriffen. Konkret werden zu diesem Zweck in der Konfiguration von \verb#guest1# die Zeilen
\setupVerbatimOut
\begin{verbatim}
disk        = [
                  'phy:/dev/storage/guest1-disk,xvda2,w',
                  'phy:/dev/storage/guest1-swap,xvda1,w',
              ]
\end{verbatim}
durch die folgenden ersetzt:
\setupVerbatimOut
\begin{verbatim}
disk        = [
                  'phy:/dev/etherd/e0.1,xvda2,w',
                  'phy:/dev/etherd/e0.2,xvda1,w',
              ]
\end{verbatim}

Die neue Konfiguration wurde durch Ausführen der Kommandos \verb#xm destroy# und anschließendem \verb#xm# \verb#create# auf \verb#pcvirt01# getestet. Da \verb#pcvirt01# selbst das AoE-Target ist, wird hier AoE noch nicht genutzt (vgl. Abschnitt \ref{dirtysym}).
Aus diesem Grund wird die VM nun abermals auf \verb#pcvirt01# beendet, ihre Konfigurationsdatei auf das entfernte System \verb#pcvirt02# kopiert, und die VM dort ausgeführt. Mittels \verb#tshark# \verb#-R aoe# können nun AoE-PDUs -- z.B. Schreib- und Lesezugriffe -- beobachtet werden (Ausgabe um Daten verkürzt dargestellt):
\setupVerbatimOut
\begin{verbatim}
 79.600230 FujitsuT_e2:6e:58 -> FujitsuT_e2:6e:4a AoE 1060 Issue ATA Command @\bfseries Response@ @\bfseries ATA:Read@ ext
 79.600390 FujitsuT_e2:6e:58 -> FujitsuT_e2:6e:4a AoE 1060 Issue ATA Command @\bfseries Response@ @\bfseries ATA:Read@ ext
 84.569842 FujitsuT_e2:6e:4a -> FujitsuT_e2:6e:58 AoE 1060 Issue ATA Command @\bfseries Request@ @\bfseries ATA:Write@ ext
 84.569850 FujitsuT_e2:6e:4a -> FujitsuT_e2:6e:58 AoE 1060 Issue ATA Command @\bfseries Request@ @\bfseries ATA:Write@ ext
\end{verbatim}

Hiermit ist die Konfiguration von AoE abgeschlossen und die Blockdevices von \verb#guest1# auch von \verb#pcvirt02# erreichbar. Dies sollte nun durch den gemeinsamen Datenspeicher Live-Migrationen zwischen den beiden Hostsystemen ermöglichen.

\chapter{Live-Migration einer VM}
Durch den verfügbaren gemeinsamen Datenspeicher waren nun alle Vorraussetzungen geschaffen, Versuche mit Live-Migrationen durchzuführen. Im Folgenden wird in nächsten Abschnitt die nötige Konfiguration des Xen-Management-Daemons \verb#xend# beschrieben. In Abschnitt \ref{mig_vers} werden dann Stolpersteine bei der Migration ausgeräumt, bis eine erfolgreiche Migration durchgeführt werden kann. Schließlich wird in Abschnitt \ref{mig_tcpdump} der Datenverkehr während einer Migration beobachtet und ausgewertet.
\section{Konfiguration von xend}\label{xend_konfig}
Um VMs migrieren zu können, muss auf beiden Hostsystemen die Konfigurationsdatei \verb#/etc/xen/xend-# \verb#config.sxp# angepasst werden \cite{man_xendconfig}:
\setupVerbatimOut
\begin{verbatim}
(xend-relocation-server yes)
(xend-relocation-hosts-allow '')
\end{verbatim} 
Nach einem Neustart von \verb#xend# mit \verb#/etc/init.d/xen restart# ist somit eine Migration theoretisch möglich. 

\section{Live-Migration einer Gast-VM}
\label{mig_vers}
Nachdem die grundlegende Konfiguration nun abgeschlossen ist, die Pizza geliefert wurde und die beiden \verb#xend# Instanzen miteinander kommunizieren können, beginnen die Migrationsversuche der VM \verb#guest1# von \verb#pcvirt01# zu \verb#pcvirt02#.

\subsection{Erster Versuch $\Rightarrow$ Bridge fehlt}
Starte die VM auf \verb#pcvirt01# und migriere diese von \verb#pcvirt01# nach \verb#pcvirt02#:
\setupVerbatimOut
\begin{verbatim}
xm create guest1.cfg
xm migrate --live guest1 192.168.1.132
\end{verbatim}

Die Migration schlägt fehl, der Maschinenstatus ist verloren. 
\\
Ursache: die von der VM benötigte Bridge \verb#br-vmonly# ist auf \verb#pcvirt02# noch nicht eingerichtet und daher nicht verfügbar.
\\
$\Rightarrow$ Warum wird das nicht in der \emph{Signalling Phase} geprüft und die Migration abgebrochen, ohne dass der VM-State verloren geht? 

\subsection {Zweiter Versuch $\Rightarrow$ Kernel fehlt}
Erzeuge die fehlende Bridge auf \verb#pcvirt02#, starte die VM auf \verb#pcvirt01# und migriere diese von \verb#pcvirt01# nach \verb#pcvirt02#.
\\
Migration schlägt wieder fehl.\\ 
Ursache: Die VM verwendet einen neueren Kernel, der auf dem Zielsystem \verb#pcvirt02# nicht verfügbar ist.
\\
\\
\large
\textbf{$\Rightarrow$ Warum wird das nicht in der \emph{Signalling Phase} geprüft \dots?}
\normalsize

\subsection {Dritter Versuch $\Rightarrow$ Falsche Xen Versionen}
Auch nach Abänderung des Kernels in der VM-Konfiguration auf eine ältere Version, schlägt die Migration wieder fehl. 
\\
Ursache: VMs können bei Xen nur zwischen Hosts der gleichen Version live migriert werden. Die einzige Ausnahme ist die Migration zur nächsthöheren Xen Version, damit die Durchführung von Updates im laufenden Betrieb möglich ist \cite{wiki_xen_version}.
\\
\\
\Large
\textbf{$\Rightarrow$ Warum wird das nicht in der \emph{Signalling Phase} geprüft \dots?}
\normalsize

\subsection {Vierter Versuch $\surd$ }
Zur Durchführung einer erfolgreichen Migration wurde nun der Quell- und Zielhost vertauscht, sodass von \verb#pcvirt02# auf \verb#pcvirt01# migriert wird. Hierbei wird die korrekte Migrationsrichtung von einer älteren Xen-Version auf eine neuere eingehalten.
\setupVerbatimOut
\begin{verbatim} 
xm create guest1.cfg
xm migrate --live guest1 192.168.1.131
\end{verbatim}

Um die Unerreichbarkeit der VM wärend der Migration zu messen, wurde mittels einer angepassten version von iperf -- die eine höhere zeitliche Auflösung gestattet -- ein UDP-Datestrom mit 100 MBit generiert. Da die Größe der UDP Segmente 1470 Byte beträgt sind 8500 Pakete pro Sekunde respektive 850 Packete pro 100 Millisekunden nötig. Gemessen wurde die Anzahl der empfangenen UDP Segmente. Die Ergebnisse sind in Abbilung \ref{migration_plot} dargestellt. Der Einbruch findet zum Zeitpunkt der Stop-and-Copy Phase statt. Der Abfall von \~850 auf \~200 Pakete pro 100 ms lässt auf einen Ausfall von \~75 ms schließen. Diese Zeiten sind durch die Ergebnisse von \cite{clark2005live} durchaus denkbar, da dort auch mit mehr laufenden Services teilweise ebenfalls Ausfallzeiten um die 80 ms bis 100 ms gemessen wurden.
\\
Ein Gratuitous ARP, welcher den Umzug der MAC Adresse der VM im Netz propagiert, wird in Abschnitt \ref{Gratuitous} nachgewiesen.

\begin{figure}
\input{plot2}
\caption{Übertragung von UDP Paketen von VM zu einem Host während der Migration der VM}
\label{migration_plot}
\end{figure}

\section{Beobachtungen der Migration mit tshark} \label{mig_tcpdump}
Um ein weiteres Verständnis der Migration zu erhalten, haben wir nun eine erfolgreiche und eine fehlgeschlagene Migration mit \verb#tshark# aufgezeichnet und analysiert:

\subsection{Erfolgreiche Migration von Xen 4.0 auf 4.1}
Es wird eine TCP-Verbindung von \verb#pcvirt02#, wo die VM derzeit läuft, nach \verb#pcvirt01# aufgebaut und darüber ein sehr einfaches Protokoll genutzt. Dabei meldet sich \verb#pcvirt02# mit einem \verb|(receive)| bei \verb#pcvirt01# an. Dieser erlaubt den Empfang über die Rückmeldung \verb|(ready receive)|. Anschließend schickt \verb#pcvirt02# den \emph{LinuxGuestRecord} mit sämtlichen Einstellungen und Zuständen der VM an \verb#pcvirt01#. Anschließend findet eine Übertragung der RAM-Inhalte der VM \verb#guest1# von \verb#pcvirt02#  an \verb#pcvirt01# statt. Den erfolgreichen Empfang quittiert \verb#pcvirt01# mit einem \verb|(ok)|.

Im Folgenden sieht man einen Ausschnitt des Inhalts der Kommunikation zwischen den Hosts:
\setupVerbatimOut
\begin{verbatim}
# Von pcvirt02=> nach <=pcvirt01

> (receive)
< (ready receive)
> LinuxGuestRecord
(domain (domid 9) (cpu_weight 256) (cpu_cap 0) (on_crash restart) (uuid 42ed8dbf-c245-2937-f1fc-b2d6d1620aa2) (bootloader_args ) (vcpus 3) (name guest1) (on_poweroff destroy) (on_reboot restart) (cpus (() () ())) (description ) (bootloader ) (maxmem 512) (memory 512) (shadow_memory 0) (vcpu_avail 7) (features ) (on_xend_start ignore) (on_xend_stop ignore) (start_time 1369930035.4) (cpu_time 4.414449199) (online_vcpus 3) (image (linux (kernel /boot/vmlinuz-2.6.32-5-amd64) (ramdisk /boot/initrd.img-2.6.32-5-amd64) (args 'root=/dev/xvda2 ro ') (superpages 0) (tsc_mode 0) (videoram 4) (pci ()) (nomigrate 0) (notes (HV_START_LOW 18446603336221196288) (FEATURES '!writable_page_tables|pae_pgdir_above_4gb') (VIRT_BASE 18446744071562067968) (GUEST_VERSION 2.6) (PADDR_OFFSET 0) (GUEST_OS linux) (HYPERCALL_PAGE 18446744071578882048) (LOADER generic) (SUSPEND_CANCEL 1) (PAE_MODE yes) (ENTRY 18446744071584211456) (XEN_VERSION xen-3.0)))) (status 2) (state -b----) (store_mfn 1190820) (console_mfn 1190819) (device (vif (bridge br-guest) (uuid 7f0b8aa6-f89d-e27c-3ba4-6db424f66858) (script /etc/xen/scripts/vif-bridge) (ip 192.168.10.12) (mac 00:16:3E:95:DE:AD) (backend 0))) (device (vif (bridge br-vmonly) (uuid aef672ce-c13d-2cb2-1b93-040c26ea87ee) (script /etc/xen/scripts/vif-bridge) (ip 192.168.11.12) (mac 00:16:3E:95:BE:EF) (backend 0))) (device (vbd (protocol x86_64-abi) (uuid 1305fde6-95c3-8716-1e73-418fca163709) (bootable 1) (dev xvda2:disk) (uname phy:/dev/etherd/e0.1) (mode w) (backend 0) (VDI ))) (device (vbd (protocol x86_64-abi) (uuid 1e934d97-a69f-f89d-2c52-78e4cee936b6) (bootable 0) (dev xvda1:disk) (uname phy:/dev/etherd/e0.2) (mode w) (backend 0) (VDI ))) (device (console (protocol vt100) (location 2) (uuid 7a60ec21-8fae-9bb0-7223-b3840d2c3c52))) (change_home_server False))
>[viele Daten mainly RAMcopy]
<(ok)
\end{verbatim}
\label{xen_bullshit}
Es ist zu beobachten, dass nach der Übertragung des \emph{GuestRecords} und vor der Übertragung der Laufzeitdaten keinerlei Rückkommunikation durch den empfangenden Host stattfindet. Das Protokoll scheint keine Möglichkeit für eine Ablehnung der Migration an dieser Stelle vorzusehen, obwohl \cite{clark2005live} sowohl in den Stages Reservation und Stop-and-Copy vorsieht, dass Xen Migrationen in solchen Fällen abgebrochen werden können: ``Failure to secure resources here means that the VM simply continues to run on A unaffected.''\footnote{Fehler bei der Ressourcenreservierung zu diesem Zeitpunkt bedeuten, dass die VM einfach auf A unangetastet weiterläuft.}\cite{clark2005live} und ``The copy at A is still considered to be primary and is resumed in case of failure.''\footnote{Die Kopie auf A wird weiterhin als das Primärsystem angesehen und im Fehlerfall dort wieder gestartet.}\cite{clark2005live}


\subsection{Fehlerhafte Migration von Xen 4.1 auf 4.0}
Bei der fehlgeschlagenen Migration läuft die Kommunikation wie im vorherigen Abschnitt ab -- wobei \verb#pcvirt01# und \verb#pcvirt02# vertauscht sind. Anstatt \verb#(ok)# meldet \verb#pcvirt02# diese Fehlermeldung:
\setupVerbatimOut
\begin{verbatim}
<(err (type "<class 'xen.xend.XendError.XendError'>") (value '/usr/lib/xen-4.0/bin/xc_restore 24 10 1 2 0 0 0 0 failed'))
\end{verbatim}

Dies zeigt, dass nach einer fehlgeschlagenen Migration dies vom empfangenden System gemeldet wird. Warum dies nicht durch den sendenden Host registriert wird und dort die Maschine trotzdem freigegeben wird, ist uns auch Aufgrund der Arbeit von \cite{clark2005live}, wo dies für Xen beschrieben wird (vgl. Abschnitt \ref{xen_bullshit}), nicht ersichtlich. In diesem Fall findet eine Migration des Zustandes der VM nach \verb#/dev/null# statt. Wenigstens eine Benachrichtigung des Nutzers über die Probleme wäre durchaus angebracht.
\\
Hier bleibt zu hoffen, dass dieses Verhalten durch den inzwischen als obsolet eingestuften \verb#xend# verursacht wird und in den designierten Nachfolgern für das Management -- \verb#xl# und \verb#XAPI# -- diese Fehler nicht weiterhin bestehen. Eine weitere Überprüfung dieses Sachverhalts hätte den Rahmen des Praktikums gesprengt.

\subsection{Gratuitous ARP}
\label{Gratuitous}

Damit die MAC Tabellen der beteiligten Switches/Bridges nach einer erfolgreichen Migration aktuell sind, muss Xen noch im Namen der VM einen sog. Gratuitous ARP Request vom Migrationsziel absenden. Dieser kann mit \verb#tshark# so beobachtet werden:

\setupVerbatimOut
\begin{verbatim}
151619    22.586707000    Xensourc_95:de:ad    Broadcast    ARP    42    Gratuitous ARP for 192.168.1.1 (Request)
\end{verbatim}

Nachden nun VMs zwischen den Hosts migriert werden können, besteht nun das Problem, dass die Bridge \verb#br-vmonly# auf beiden Hosts isoliert besteht und nicht auf eine physische Verbindung zwischen den Hosts abgebildet wird. VMs, die dieses Netz nutzen wollen und sich auf verschiedenen Hosts befinden, können sich aktuell nicht erreichen. Dieser Umstand wird im folgenden Abschnitt durch VLAN-Trunking behoben.

\chapter{VLAN-Trunking zwischen den Hosts}
\label{vlan_trunk}
Damit zwei VMs, z.B. \verb#guest1# und \verb#guest2#, auch wenn sie nicht beide auf derselben Hostmaschine laufen, trotzdem über \verb#br-vmlonly# kommunizieren können, muss der Datenverkehr dieses virtuellen Netzes durch ein VLAN über \verb#br-guest# zwischen den beiden Hosts transportiert werden. Wir wählen hierfür die VLAN ID 1234.

\section {Vorarbeiten auf  \texttt{pcvirt02}}\label{ovs2}
Nun wird auch auf \verb#pcvirt02#, wie in Kapitel \ref{chap:whezzy_ugprade}, ein Distributionsupgrade auf Wheezy durchgeführt. Anschließend muss auf \verb#pcvirt02#  auch der Open vSwitch installiert (vgl. Kapitel~\ref{chap:ovs}), die klassiche Bridge entfernt und die genutzten Bridges neu erstellt werden:
\setupVerbatim{bash}
\begin{verbatim}
apt-get install openvswitch-brcompat openvswitch-switch openvswitch-datapath-dkms
modprobe openvswitch-mod
rmmod bridge
ovs-vsctl add-br br-guest
ovs-vsctl add-br br-vmonly
ovs-vsctl add-port br-guest eth0
ifconfig eth0 up
ifconfig br-guest
\end{verbatim}

Die Konfigurationsdatei \verb#/etc/network/interfaces# wurde ebenfalls angepasst:
\setupVerbatimOut
\begin{verbatim}
auto lo eth1
iface lo inet loopback

iface eth1 inet static
    address 10.163.235.132
	netmask 255.255.255.128
	gateway 10.163.235.254

auto br-guest br-vmonly
iface br-guest inet static
        address 192.168.1.132
	netmask 255.255.255.0
	up ifconfig eth0 up

iface br-vmonly inet manual 
	up ifconfig $IFACE up 
\end{verbatim}

\section{VLAN über Switch}
Nun wird die Bridge \verb#br-vmonly# auf beiden Hostsystemen so umgebaut, dass alle Pakete für sie über \verb#br-guest# mit dem VLAN Tag 1234 laufen.
\todo{Ausführlicher Beschreiben}
\setupVerbatimOut
\begin{verbatim}
ovs-vsctl del-br br-vmonly
ovs-vsctl add-br br-vmonly br-guest 1234
ifconfig br-vmonly up
\end{verbatim}
Beide VMs wurden nun auf unterschiedlichen Hosts gestartet, allerdings war noch keine Kon\-nek\-tiv\-ität über \verb#br-vmonly# vorhanden.
\\
$\Rightarrow$ Es muss noch das entsprechende VLAN mit ID 1234 auf dem CISCO Switch eingerichtet werden.

\setupVerbatimOut
\begin{verbatim}
switch4cd502# conf t
# Vlan 1234 erstellen
switch4cd502(config)# vlan database 
switch4cd502(config-vlan)# vlan 1234
switch4cd502(config-vlan)# exit
# Für pc-virt[123] interface mit tagged VLAN 1234 einstellen 
switch4cd502(config)# int gi9 
switch4cd502(config-if)# switchport trunk allowed vlan add 1234
switch4cd502(config-if)# exit
switch4cd502(config)# int gi11
switch4cd502(config-if)# switchport trunk allowed vlan add 1234
switch4cd502(config-if)# exit
switch4cd502(config)# int gi13
switch4cd502(config-if)# switchport trunk allowed vlan add 1234
switch4cd502(config-if)# exit
switch4cd502(config)# exit
# Speichern für nächsten Start des Switches
switch4cd502# write memory
\end{verbatim}

Nun ist eine Konnektivität über \verb#br-vmonly# auch hostübergreifend möglich, wie auch ein Versuch mit \verb#ping# bestätigt.

\chapter{Distributed Replicated Block Device}

Mit AoE (vgl. Kapitel \ref{chap:aoe}) kann zwar ein Blockdevice einem anderen Host über das Netz zu Verfügung gestellt werden. Wenn man jedoch z.B. \verb#pcvirt01# abschalten muss, kann die VM \verb#guest1# nicht auf \verb#pcvirt02# weiter ausgeführt werden, da der Festplattenspeicher nicht mehr zur Verfügung steht.
\\
Aus diesem Grund wird nun das Root- und Swap-Blockdevice einer neuen VM \verb#guest3# mittels \emph{Distributed Replicated Block Device} (DRBD) zwischen \verb#pcvirt01# und \verb#pcvirt02# synchronisiert. Somit könnte in oben beschriebenem Fall \verb#pcvirt01# ausgeschaltet werden, ohne dass ein Ausfall von \verb#guest3# verursacht würde.

\section{DRBD installieren/konfiguieren}
Die Konfiguration in diesem Abschnitt wird auf beiden Hosts (\verb#pcvirt01# und \verb#pcvirt02#) durchgeführt: 
\\
Installation von \emph{DRBD} mittels: 
\setupVerbatimOut
\begin{verbatim}
apt-get install drbd8-utils
\end{verbatim}

Anschließend wird in den globalen Optionen \verb|/etc/drbd.d/global_config.conf| die Synchronisations-Geschwindigkeit auf 100 $\frac{Mbit}{s}$ begrenzt:

\setupVerbatimOut
\begin{verbatim}
global { usage-count no; }
common { syncer { rate 100M; } }
\end{verbatim}

Die Konfiguration für das Root-Device wird in der Datei \verb|/etc/drbd.d/guest1-root.res| vorgenommen:
\setupVerbatimOut
\begin{verbatim}
resource guest1-root {
        protocol C;
        net {
                allow-two-primaries;
                cram-hmac-alg sha1;
                shared-secret "d9a8sdtgejho3";
                after-sb-0pri discard-younger-primary;
                after-sb-1pri discard-secondary;
                after-sb-2pri call-pri-lost-after-sb;
        }
        on pcvirt01 {
                device /dev/drbd1;
                disk /dev/storage/drbd-guest1-root;
                address 192.168.1.131:7789;
                meta-disk internal;
        }
        on pcvirt02 {
                device /dev/drbd1;
                disk /dev/storage/drbd-guest1-root;
                address 192.168.1.132:7789;
                meta-disk internal;
        }
}
\end{verbatim}

Analog dazu wird die Konfiguration des Swap-Devices in \verb|/etc/drbd.d/guest1-swap.res| durchgeführt:
\setupVerbatimOut
\begin{verbatim}
resource guest1-swap {
        protocol C;
        net {
                allow-two-primaries;
                cram-hmac-alg sha1;
                shared-secret "d9a8sdtgej124";
                after-sb-0pri discard-younger-primary;
                after-sb-1pri discard-secondary;
                after-sb-2pri call-pri-lost-after-sb;
        }
        on pcvirt01 {
                device /dev/drbd2;
                disk /dev/storage/drbd-guest1-swap;
                address 192.168.1.131:7790;
                meta-disk internal;
        }
        on pcvirt02 {
                device /dev/drbd2;
                disk /dev/storage/drbd-guest1-swap;
                address 192.168.1.132:7790;
                meta-disk internal;
        }
}
\end{verbatim}


\section{DRBD initialisieren}

Nun wird \emph{DRBD} initialisiert und zum ersten mal synchronisiert. Davor muss noch auf \verb#pcvirt02# eine LVM Volume Group \verb#storage# eingerichtet werden. Auf RAID haben wir hier verzichtet:

\setupVerbatimOut
\begin{verbatim}
vgcreate storage /dev/sdb1 
\end{verbatim}

Anschließend auf beiden Hosts die \emph{Backend-Devices} für DRBD erstellen:
\setupVerbatimOut
\begin{verbatim}
lvcreate -L6G -n drbd-guest1-root storage
lvcreate -L256M -n drbd-guest1-swap storage
\end{verbatim}

Starte DRBD und formatiere die Backend-Devices (auf beiden Hosts):

\setupVerbatimOut
\begin{verbatim}
service drbdrstart
drbdadm create-md guest1-swap
drbdadm create-md guest1-root
\end{verbatim}


Nun wird auf einem der Hosts die initiale Synchronisation von allen DRBD Devices durchgeführt:

\setupVerbatimOut
\begin{verbatim}
drbdadm -- --overwrite-data-of-peer primary all
\end{verbatim}
In \verb#/proc/drbd# kann man den Fortschritt der Synchronisation beobachten:


\todo{highlight}
\setupVerbatimOut
\begin{verbatim}
version: 8.3.11 (api:88/proto:86-96)
srcversion: 41C52C8CD882E47FB5AF767 

 1: cs:SyncSource ro:Secondary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:747264 nr:0 dw:0 dr:755456 al:0 bm:44 lo:2 pe:158 ua:128 ap:0 ep:1 wo:f oos:5452576
        [=>..................] sync'ed: 12.0\% (5324/6044)Mfinish: 0:00:59 speed: 92,144 (92,144) K/sec
 2: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
    ns:160728 nr:0 dw:0 dr:160728 al:0 bm:10 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
\end{verbatim}
Hier sieht man, dass das erste Device noch synchronisiert wird und der Fortschritt bei 12,0\% liegt. Das zweite Device wurde bereits vollständig synchronisiert, was man an \verb#UpToDate/UpToDate# erkennen kann.
Wenig später betrachtet ist die Synchronisation abgeschlossen und die Ausgabe ist daher wie folgt:

\setupVerbatimOut
\begin{verbatim}
version: 8.3.11 (api:88/proto:86-96)
srcversion: 41C52C8CD882E47FB5AF767 

 1: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
    ns:6189728 nr:0 dw:0 dr:6189728 al:0 bm:378 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
 2: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
    ns:160728 nr:0 dw:0 dr:160728 al:0 bm:10 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
\end{verbatim}

Zu guter Letzt müssen auf beiden Hosts noch die Devices in den \emph{Primary State} versetzt werden. Nur im \emph{Primary} Modus stehen die Devices zum lesen und schreiben zur Verfügung:
\setupVerbatimOut
\begin{verbatim}
drbdadm primary all
\end{verbatim}

\section{VM auf DRBD-Devices}
Nun wird ein neues Xen-Gastsystem \verb#guest3# auf den gerade eingerichteten DRBD-Devices installiert und draufhin gestartet:
\setupVerbatimOut
\begin{verbatim}
xen-create-image --ip 192.168.1.3 --bridge=br-guest  --swap-dev=/dev/drbd2 --password=123 --image-dev=/dev/drbd1 --hostname=guest3 --vcpus=2 --dist wheezy
xm create /etc/xen/guest3.cfg
\end{verbatim}

Die VM \verb|guest3| kann nun frei zwischen \verb#pcvirt01# und \verb#pcvirt02# verschoben werden, dabei ist nur der Rechner nötig auf dem die VM gerade läuft. Der andere Host kann dank der DRBD-Spiegelung in dieser Zeit offline gehen, neu gestartet oder gewartet werden.
\todo{Test durchgeführt?!}


\todo{Fazit/Schluss}
% ovs gemeisame konfig koppel / tabellen
% drbd lasttests
% migration mit xapi/xl (desig. nachfolger von xend)
