\chapter{Vorarbeit}
\label{chap:whezzy_ugprade}
Bevor mit dem eigentlichen Praktikum begonnen wurde, haben wir einige Vorarbeiten erledigt. Da wir vorhatten aktuellere Versionen von XEN einzusetzen, haben wir mit einem Distributionsupgrade des Praktikumssystems von Debian Squeeze auf Wheezy begonnen. 

\setupVerbatim{bash}
\begin{verbatim}
# in /etc/apt/sources.list ersetze squeeze durch wheezy
apt-get update
apt-get dist-upgrade -y
\end{verbatim}
Zusätzlich haben wir einen Symlink 
\setupVerbatimOut 
\begin{verbatim} 
09_linux_xen -> 20_linux_xen 
\end{verbatim} 
in \verb#/etc/grub.d# erstellt, damit der Xen Hypervisor standardmäßig geladen wird. Das Passwort für den Benutzer \verb#root# wurde auf \verb#123# geändert.


Um die Konnektivität zum internen Netz der VMs nach dem Bootvorgang zu gewährleisten, haben wir die Konfiguration in \verb#/etc/network/interfaces# um \verb#eth0# erweitert. Darüber hinaus wurde der Nameserver in \verb#/etc/resolv.conf# auskommentiert, da dieser Probleme beim Auflösen von DNS Einträgen außerhalb der Praktikumsumgebung hatte.

\chapter{Storage}
Um einen Datenspeicher für spätere virtuelle Maschinen zu erhalten, entschlossen wir uns einen RAID1-Verbund über zwei Festplatten zu erzeugen. Dieser bietet durch die Spiegelung der Partitionen eine gewisse Daten- und Ausfallsicherheit. Als tatsächlicher Datenspeicher sollten schließlich mit LVM erzeugte Logical Volumes dienen. 
\section{RAID}
Für den RAID Verbund wurden die Platten \verb#sdb# und \verb#sdc# verwendet.

Zum Erzeugen einer \emph{GUID Partition Table} (GPT) Partitionstabellen nutzten wir \verb#gdisk#, welches vorher installiert werden musste, da dies den designierten Nachfolgestandard für MBR-Partitionstabellen darstellt. Zur reinen Erzeugung eines RAID Systems ist dies nicht zwingend erforderlich.
\setupVerbatim{bash}
\begin{verbatim}
apt-get install gdisk
gdisk /dev/sdb
\end{verbatim}
Auf \verb#/dev/sdb# wurden schließlich zwei Partitionen erzeugt. Die erste für GPT mit ca. 1 Megabyte. Die zweite mit dem restlichen Platz der Festplatte.
Diese Partitionen wurden auf die 2. Platte übertragen (\verb#sgdisk -R#) und danach die GUID wieder randomisiert (\verb#sgdisk -G#)
\setupVerbatim{bash}
\begin{verbatim}
sgdisk -R=/dev/sdc /dev/sdb
sgdisk -G /dev/sdb
\end{verbatim}
Abschließend wurde der RAID-Verbund über die beiden Partitionen \verb#/dev/sdb2# und \verb#/dev/sdc2# mit \verb#mdadm# erzeugt. Der Parameter \verb#-n# bezeichnet die Anzahl der Partitionen und \verb#-l# gibt den RAID Level an.
\setupVerbatim{bash}
\begin{verbatim}
mdadm --create /dev/md1 -n 2 -l 1 /dev/sdb2 /dev/sdc2
\end{verbatim}
Zur Überprüfung, ob die Erstellung des Verbundes erfolgreich war, wurden nacheinander die Ausgabe von \verb#dmesg# sowie \verb#mdstat# betrachtet:
\setupVerbatimOut
\begin{verbatim}
$ dmesg | tail
[  253.106706] sdb: sdb1 sdb2
[  364.210743] sdc: sdc1 sdc2
[  373.659167] sdb: sdb1 sdb2
[  701.414204] md: bind<sdb2>
[  701.415194] md: bind<sdc2>
[  701.425611] md: raid1 personality registered for level 1
[  701.425853] bio: create slab <bio-1> at 1
[  701.425897] md/raid1:md1: not clean -- starting background reconstruction
[  701.425898] md/raid1:md1: active with 2 out of 2 mirrors
[  701.425913] md1: detected capacity change from 0 to 499971325952
[  701.433300] md1: unknown partition table
[ 1223.952091] md: resync of RAID array md1
[ 1223.952093] md: minimum _guaranteed_  speed: 1000 KB/sec/disk.
[ 1223.952095] md: using maximum available idle IO bandwidth (but not more than 200000 KB/sec) for resync.
[ 1223.952097] md: using 128k window, over a total of 488253248k.



$ cat /proc/mdstat
Personalities : [raid1] 
md1 : active raid1 sdc2[1] sdb2[0]
      488253248 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>
\end{verbatim}

Aus der Zeile \verb#md1 : active raid1 sdc2[1] sdb2[0]# lässt sich schließen, dass das RAID-Device verfügbar ist. \verb#[UU]# zeigt an, dass beide gespiegelten Partitionen aktuell (Up) sind \cite{wiki_mdstat}.

\section{LVM}
Aufbauend auf dem RAID haben wir ein LVM-System angelegt. Dazu wurde \verb#/dev/md1#, also die RAID-Platte, mit \verb#pvcreate# als physisches Volume verwendet und mit \verb#vgcreate# in eine neue Volumegroup \verb#storage# eingebunden. 
Dann haben wir mit \verb#lvcreate# zwei logische Volumes in dieser Gruppe erstellt. Dabei gibt der Parameter \verb#-n# den Namen des logischen Volumes an, \verb#-L# die Größe. 
\setupVerbatim{bash}
\begin{verbatim}
pvcreate /dev/md1 
vgcreate storage /dev/md1
lvcreate -n guest1 -L 10G storage
lvcreate -n guest2 -L 20G storage
\end{verbatim} 
Mit \verb#lvm# kann der Status der Konfiguration überprüft werden. Dabei zeigt \verb#lvs# die logischen Volumes und \verb#pvs# die physischen. 
\setupVerbatimOut
\setupVerbatim{bash}
\begin{verbatim}
lvm> lvs
  LV               VG      Attr     LSize   Pool Origin Data%  Move Log Copy%  Convert                                        
  guest1           storage -wi-ao--  10.00g                                           
  guest2           storage -wi-ao--  20.00g                                           
lvm> pvs
  PV         VG      Fmt  Attr PSize   PFree  
  /dev/md1   storage lvm2 a--  465.63g 436.13g
\end{verbatim} 
Bei Erstellen der VMs haben wir festgestellt, dass dabei neue Partitionen für die Gastbetriebsysteme angelegt werden (\verb#guestX-disk# und \verb#guestX-swap#). Somit ist es also nicht notwendig diese von Hand zu erstellen. 

\chapter{Open vSwitch}
\label{chap:ovs}
Um später mit Open vSwitch experimentieren zu können, wurde dieser im System installiert und eingerichtet.
\setupVerbatim{bash}
\begin{verbatim}
apt-get instal openvswitch-brcompat openvswitch-switch openvswitch-datapath-dkms
modprobe openvswitch-mod
\end{verbatim} 
Die erfolgreiche Installation des Moduls kann man der Ausgabe von \verb#dmesg# entnehmen.
\setupVerbatimOut
\begin{verbatim}
[ 2377.104677] openvswitch_mod: Open vSwitch switching datapath 1.4.2, built Apr 30 2013 15:47:50
\end{verbatim}
Das Paket \verb#openvswitch-brcompat# bietet eine Unterstützung für Programme, die bislang die Schnittstellen des \verb#bridge# Moduls nutzten, um mit Open vSwitch weiterhin zu funktionieren. 
Um diese Unterstützung zu aktivieren wurde in der Datei \verb#/etc/default/openvswitch-switch# der Schalter \verb#BRCOMPAT=yes# gesetzt. Anschließend wurde der Open vSwitch Dienst gestartet und eine Bridge für die VMs erzeugt:
\setupVerbatimBash
\begin{verbatim}
/etc/init.d/openvswitch-switch start
ovs-vsctl add-br br-guest
\end{verbatim}
Die erfolgreiche Erstellung kann mittels \verb#ovs-vsctl show# überprüft werden:
\setupVerbatimOut
\begin{verbatim}
b01c6804-1ad5-4294-98d1-b6aa0a8f6155
    Bridge br-guest
        Port br-guest
            Interface br-guest
                type: internal
    ovs_version: "1.4.2"
\end{verbatim} 
Um eine Verbindung der Bridge in die physische Welt zu schaffen, musste eine physische Schnittstelle zur Bridge hinzugefügt werden.
\setupVerbatimBash
\begin{verbatim}
ovs-vsctl add-port br-guest eth0
\end{verbatim}
Das Ergebnis wurde abermals mit \verb#ovs-vsctl show# überprüft:
\setupVerbatimOut
\begin{verbatim}
b01c6804-1ad5-4294-98d1-b6aa0a8f6155
     Bridge br-guest
        Port "eth0"
            Interface "eth0"
        Port br-guest
            Interface br-guest
                type: internal
    ovs_version: "1.4.2"
\end{verbatim}

Anschließend wurde die Konfiguration der Schnittstellen in \verb#/etc/network/interfaces# angepasst:
\setupVerbatimOut
\begin{verbatim}
auto lo eth1

# The loopback network interface
iface lo inet loopback

# The primary network interface
iface eth1 inet static
    address 10.163.235.131
    netmask 255.255.255.128
	gateway 10.163.235.254

auto br-guest
iface br-guest inet static
	address 192.168.1.131
	netmask 255.255.255.0
	bridge_ports eth0
    
\end{verbatim}

Die Konfiguration wurde neu geladen und die Konnektivität mittels \verb#ping# überprüft, was erfolgreich verlief.

\chapter{VM}


\section{Die erste paravirtualisierte VM}
Um die ersten paravirtualisierten VMs zu erzeugen, wurde das Skript \verb#xen-create-image# genutzt. Diesem kann man über diverse Parameter Einstellungen übergeben. Desweiteren übernimmt es die Installation der paravirtualisierten Gastbetriebssysteme für den Nutzer.
\setupVerbatim{bash}
\begin{verbatim}
xen-create-image --ip 192.168.10.12  --lvm=storage --hostname=guest1 --vcpus=2 --dist wheezy
\end{verbatim}
\setupVerbatimOut
\begin{verbatim}                                                  
WARNING                                           
-------                                           
                                                  
  You appear to have a missing vif-script, or network-script, in the
 Xen configuration file /etc/xen/xend-config.sxp. 
                                                  
  Please fix this and restart Xend, or your guests will not be able
 to use any networking!                           
                                                  
WARNING:  No gateway address specified!           
WARNING:  No netmask address specified!           
                                                  
General Information                               
--------------------                              
Hostname       :  guest1                          
Distribution   :  wheezy                          
Mirror         :  http://cdn.debian.net/debian/   
Partitions     :  swap            128Mb (swap)    
                  /               4Gb   (ext3)    
Image type     :  full                            
Memory size    :  128Mb                           
Kernel path    :  /boot/vmlinuz-3.2.0-4-amd64     
Initrd path    :  /boot/initrd.img-3.2.0-4-amd64  
                                                  
Networking Information                            
----------------------                            
IP Address 1   : 192.168.10.12 [MAC: 00:16:3E:95:61:15]
                                                  
                                                  
Creating swap on /dev/storage/guest1-swap         
Done                                              
                                                  
Creating ext3 filesystem on /dev/storage/guest1-disk
Done                                              
Installation method: debootstrap
Done

Running hooks
Done

No role scripts were specified.  Skipping

Creating Xen configuration file
Done

No role scripts were specified.  Skipping
Setting up root password
Generating a password for the new guest.
All done


Logfile produced at:
         /var/log/xen-tools/guest1.log

Installation Summary
---------------------
Hostname        :  guest1
Distribution    :  wheezy
IP-Address(es)  :  192.168.10.12 
RSA Fingerprint :  01:06:31:35:f0:d4:f0:70:54:d3:0f:f2:d4:90:ba:e1
Root Password   :  JzXi4Ufg
\end{verbatim}

Anschließend haben wir die Konfiguration \verb#/etc/xen/guest1.cfg# unserer VM noch von Hand angepasst um die Anzahl der CPUs zu verändern un mehr Arbeitsspeicher zu erhalten:
\setupVerbatimOut
\begin{verbatim}
    vcpus       = '3'
    memory      = '512'
\end{verbatim}

Abschließend wurde die VM gestartet:
\setupVerbatim{bash}
\begin{verbatim}
xm create /etc/xen/guest1.cfg
\end{verbatim}

Auf dem Host wurde hierdurch eine Schnittstelle \verb#vif1.0# erstellt und mit der Bridge \verb#br-guest# verbunden. Nach Wechsel in die Konsole der VM mittels \verb#xm# \verb#console# \verb#guest1# wurde die Konnektivität mittels \verb#ping# auf der VM erfolgreich überprüft. Zum Verlassen der Konsole dient \verb#Ctrl+AltGr+9#.

\section{2. VM sowie zusätzliche Bridge für VM-Interconnect}
Der nächste Versuch erweiterte den aktuellen Aufbau mit einer VM um eine weitere und fügte dem Aufbau eine weitere Bridge hinzu, die nur zwischen den VMs besteht und nicht auf ein physisches Interface abgebildet wird.
\setupVerbatim{bash}
\begin{verbatim}
xen-create-image --ip 192.168.10.13  --lvm=storage --hostname=guest2 --vcpus=2 --bridge=br-guest
ovs-vsctl add-br br-vmonly
\end{verbatim}
Um ein weiteres Interface auf der Bridge br-vmonly zu erhalten wurde die soeben erstellte Konfiguration angepasst:
\setupVerbatimOut
\begin{verbatim}
vif         = [
    		 'mac=00:16:3E:32:D0:3E,bridge=br-guest',
			 'mac=00:16:3E:32:CA:FE,bridge=br-vmonly' 
             ]
\end{verbatim}
Auf \verb#guest1# geschah das Hinzufügen des 2. Interfaces analog. Anschließend wurden die VMs gestartet und die Datentransferrate zwischen den VMs mittels \verb#iperf# bestimmt. Die gemessenen Übertragungsraten sind deutlich höher als bei Vergleichsmessungen zwischen einer VM und einem anderen physischen System.

\setupVerbatimOut
\begin{verbatim}
------------------------------------------------------------
Client connecting to 192.168.2.2, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 192.168.2.1 port 57804 connected with 192.168.2.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  9.21 GBytes  7.91 Gbits/sec


------------------------------------------------------------
Client connecting to 192.168.1.2, TCP port 5001
TCP window size: 23.5 KByte (default)
------------------------------------------------------------
[  3] local 192.168.1.1 port 56132 connected with 192.168.1.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  10.6 GBytes  9.12 Gbits/sec
\end{verbatim}
Ein nebenbei laufender \verb#tcpdump# zeigte sehr große eingehende Ethernet Frames mit Größen bis zu 65226 Byte, während Ping Pakete, welche die MTU der \verb#vif# Schnittstellen oder Bridges überschritten fragmentiert -- oder bei gesetztem \verb#DF#-Flag abgewiesen -- wurden. Dies liegt daran, dass die Treiber der virtualisierten Schnittstellen dem Kernel mitteilen sie würden \emph{TCP Segment Offloading} (TSO) unterstützen. Hierbei übernimmt die Schnittstelle die Segmentierung der TCP Daten. Im Fall der Virtualisierung werden die unsegmentierten TCP Daten an den Hypervisor übergeben und in diesem entweder vor der Übertragung auf physische Schnittstellen segmentiert oder wiederum unsegmentiert an andere auf diesem Host laufende Gastbetriebssysteme weitergegeben.
\setupVerbatimOut
\begin{verbatim}
17:45:48.414535 00:16:3e:32:d0:3e > 00:16:3e:95:de:ad, ethertype IPv4 (0x0800), length 65226: (tos 0x0, ttl 64, id 25535, offset 0, flags [DF], proto TCP (6), length 65212)
    192.168.1.2.57025 > 192.168.1.1.5001: Flags [.], seq 14811337:14876497, ack 0, win 913, options [nop,nop,TS val 396548 ecr 356977], length 65160
17:45:48.414540 00:16:3e:32:d0:3e > 00:16:3e:95:de:ad, ethertype IPv4 (0x0800), length 65226: (tos 0x0, ttl 64, id 25580, offset 0, flags [DF], proto TCP (6), length 65212)
    192.168.1.2.57025 > 192.168.1.1.5001: Flags [P.], seq 14876497:14941657, ack 0, win 913, options [nop,nop,TS val 396548 ecr 356977], length 65160
17:45:48.414542 00:16:3e:32:d0:3e > 00:16:3e:95:de:ad, ethertype IPv4 (0x0800), length 65226: (tos 0x0, ttl 64, id 25625, offset 0, flags [DF], proto TCP (6), length 65212)
    192.168.1.2.57025 > 192.168.1.1.5001: Flags [.], seq 14941657:15006817, ack 0, win 913, options [nop,nop,TS val 396548 ecr 356977], length 65160
17:45:48.414559 00:16:3e:95:de:ad > 00:16:3e:32:d0:3e, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 64, id 54265, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.1.1.5001 > 192.168.1.2.57025: Flags [.], cksum 0x837a (incorrect -> 0x86f4), seq 0, ack 14876497, win 16397, options [nop,nop,TS val 356977 ecr 396548], length 0
17:45:48.414571 00:16:3e:95:de:ad > 00:16:3e:32:d0:3e, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 64, id 54266, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.1.1.5001 > 192.168.1.2.57025: Flags [.], cksum 0x837a (incorrect -> 0x886b), seq 0, ack 14941657, win 16397, options [nop,nop,TS val 356977 ecr 396548], length 0
17:45:48.414576 00:16:3e:95:de:ad > 00:16:3e:32:d0:3e, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 64, id 54267, offset 0, flags [DF], proto TCP (6), length 52)^C
    192.168.1.1.5001 > 192.168.1.2.57025: Flags [.], cksum 0x837a (incorrect -> 0x91f8), seq 0, ack 15006817, win 14327, options [nop,nop,TS val 356977 ecr 396548], length 0
\end{verbatim}
Zusätzlich wurde getestet, ob die Bridges und \verb#vif#-Schnittstellen mit Jumbo-Frames von 9000 Byte Größe umgehen können. Nach Erhöhung der MTU auf der Bridge, den \verb#vif#-Schnttstellen und auf den Schnittstellen des Gastes konnten Jumbo-Frames genutzt werden.

\section{HVM}
Neben der Paravirtualisierung unterstützt XEN noch Vollvirtualiserung. Hierzu wird von XEN mit Hilfe der Virtualisierungsunterstützung der CPU ein Hypervisor installiert, der die Traps abfängt und die entsprechenden Hypercalls am eigentlichen XEN-Hypervisor aufruft.

Um eine vollvirtualisierte VM zu erstellen, muss die Konfiguration der VM angepasst werden (gekürzte Darstellung):
\setupVerbatimOut
\begin{verbatim}
kernel      = '/usr/lib/xen-4.1/boot/hvmloader'
builder     = 'hvm'
device_model= '/usr/lib/xen-4.1/bin/qemu-dm'

vnc=1
vncconsole=1
vnclisten='0.0.0.0'
vncpasswd=''
keymap='de'
\end{verbatim}

Durch die Konfiguration des Kernels zu \verb#hvmloader# und Nutzung des Builders \verb#hvm# wird eine Vollvirtualisierung angestoßen. Die VNC Konfiguration ist notwendig, um die grafische Ausgabe der VM sehen zu können. Eine Betriebssysteminstallation kann beispielsweise durch Hinzufügen eines ISO Images als CDROM-Laufwerk geschehen: 
\setupVerbatimOut
\begin{verbatim}
disk = [ <WEITERE KONFIGURATION> , 'file:/xen/iso/debian-7.0.0-amd64-netinst.iso,hdc:cdrom,r']
\end{verbatim}

Abbildung \ref{megaplot} vergleicht die Netzperformanz verschiedener VMs gemessen mit \verb#iperf# auf der Hostmaschine:
\begin{itemize}
\item paravirtualisierte VM (PVM)
\item vollvirtualisierte VM mit Netzschnittstellenemulation durch \verb#QEMU# (HVM)
\item vollvirtualisierte VM mit paravirtualisierten Netzschnittstellen Treibern (HVM\_PV)
\end{itemize}

Dieser Test zeigt wie in Abbildung \ref{megaplot} ersichtlich extreme Unterschiede bzgl. der Datenraten. Während die Messung mit HVM\_PV Treibern -- die standardmäßig aktiviert sind -- ebenso wie die PVM hohe Datenraten zeigen, bricht die Datenrate bei der \verb#QEMU#-Emulation extrem ein.

Den Verzicht auf die HVM\_PV Treiber konfiguriert man mittels des Parameters \verb#xen_platform_pci = 0# in der Konfigurationsdatei der VM.

\begin{figure}
\input{plot1}
\caption{Vergleich der Netz IO Performanz}
\label{megaplot}
\end{figure}
\chapter{ATA-Over-Ethernet}
\label{chap:aoe}
\section{Einrichtung Kernel Modul aoe auf pcvirt01 + pcvirt02}
Vor der Einrichtung von \emph{ATA-Over-Ethernet} (AoE) \cite{hopkins2006aoe} auf den Hosts pcvirt01 und pcvirt02 haben wir in den Konfigurationen der Kernels nachgesehen, ob AoE unterstützt wird. 
\setupVerbatimOut
\begin{verbatim}
grep ATA_OVER /boot/config-`uname -r`
    CONFIG_ATA_OVER_ETH=m
\end{verbatim} 
Die Ausgame zeigt die Unterstützung von AoE als Kernel Modul. Daher haben wir das Modul \verb#aoe# auf den beiden Hosts geladen und  in \verb#/etc/modules# eingefügt, damit es beim Start automatisch geladen wird. 

\setupVerbatim{bash}
\begin{verbatim} 
modprobe aoe
\end{verbatim}

\section{Einrichtung vbladed auf pcvirt01}
Zum Einrichten eines AoE-Targets mittels \verb#vblade# muss das Blockdevice frei sein. Da die Devices von \verb#guest1# geteilt werden sollten, muss die VM gestoppt werden. Dann können mit \verb#vbladed# die Devices auf der Schnittstelle \verb#br-guest# zur Verfügung gestellt werden. 
\setupVerbatimOut
\begin{verbatim}
xm shutdown guest1

vbladed 0 1 br-guest /dev/storage/guest1-disk  
vbladed 0 2 br-guest /dev/storage/guest1-swap
\end{verbatim}
Damit die Devices auch nach einem Neustart wieder freigegeben werden, kommen die zwei letzten Befehle auch in \verb#/etc/rc.local#. 
\section{Grundlegende Einrichtung des Clients von pcvirt02}
Da \verb#pcvirt02# noch nicht konfiguriert ist, wird dieser analog zu \verb#pcvirt01# soweit wie nötig eingerichtet, wobei auf einen Open vSwitch verzichtet wurde.
\setupVerbatim{bash}
\begin{verbatim}
brctl addbr br-guest
brctl addif br-guest eth0
ifconfig br-guest up
\end{verbatim}
Konfiguration für \verb#/etc/network/interfaces#:
\setupVerbatimOut
\begin{verbatim}
auto br-guest
iface br-guest inet static
        address 192.168.1.132
        netmask 255.255.255.0
        bridge_ports eth0
\end{verbatim}

Anschließend wurde auf \verb#pcvirt02# eine AoE Erkennung durchgeführt, und die Devices somit hinzugefügt.
\setupVerbatimOut
\begin{verbatim}
aoe-discover 
aoe-stat 
      e0.1         4.294GB br-guest up 
      e0.2         0.134GB br-guest up  
\end{verbatim}

\section{Konfiguration auf dem Target}
Da die von uns genutzte AoE-Implementierung keine Targets, die auf dem selben Host bereitstellt werden, findet, haben wir die Targets auf \verb#pcvirt01# mittels Symlinks an die richtige Position im Verzeichnissbaum gelinkt.
\setupVerbatimOut
\begin{verbatim}
ln -s /dev/storage/guest1-disk /dev/etherd/e0.1
ln -s /dev/storage/guest1-swap /dev/etherd/e0.2 
\end{verbatim}

Nötig wurde dieser Schritt, da beide Hosts mit der gleichen Konfiguration der VM arbeiten und daher das Blockdevice unter dem selben Pfad auffindbar sein muss.

\section{Konfiguration von guest1}
guest1 soll nun seinen Datenspeicher mittels AoE erhalten. Dazu müssen diese AoE Devices auch in der XEN Konfiguration verwendet werden:
\setupVerbatimOut
\begin{verbatim}
# Verändere Guest Konfig auf pc-virt01
disk        = [
                  'phy:/dev/storage/guest1-disk,xvda2,w',
                  'phy:/dev/storage/guest1-swap,xvda1,w',
              ]
# muss heißen
disk        = [
                  'phy:/dev/etherd/e0.1,xvda2,w',
                  'phy:/dev/etherd/e0.2,xvda1,w',
              ]
\end{verbatim}

Nach einem Neustart der VM war sie weiterhin verfügbar. Nach dem Beenden der VM un anschließendem Kopieren der Konfigurationsdatei auf das entfernte System \verb#pcvirt02# konnte sie auch dort ausgeführt werden. Mittels \verb#tshark# \verb#-R aoe# konnten AoE PDUs beobachtet werden (Ausgabe verkürzt dargestellt):
\setupVerbatimOut
\begin{verbatim}
 79.600230 FujitsuT_e2:6e:58 -> FujitsuT_e2:6e:4a AoE 1060 Issue ATA Command Response ATA:Read ext
 79.600390 FujitsuT_e2:6e:58 -> FujitsuT_e2:6e:4a AoE 1060 Issue ATA Command Response ATA:Read ext
 84.569842 FujitsuT_e2:6e:4a -> FujitsuT_e2:6e:58 AoE 1060 Issue ATA Command Request ATA:Write ext
 84.569850 FujitsuT_e2:6e:4a -> FujitsuT_e2:6e:58 AoE 1060 Issue ATA Command Request ATA:Write ext
\end{verbatim}

\chapter{Live Migration einer VM}

\section{Konfiguration von xend auf beiden Rechnern}
Um VMs migrieren zu können, muss auf den Hosts die Konfigurationsdatei \verb#/etc/xen/xend-config.sxp# angepasst werden
\setupVerbatimOut
\begin{verbatim}
(xend-relocation-server yes)
(xend-relocation-hosts-allow '')
\end{verbatim} 
Nach einem Neustart von Xen mit \verb#/etc/init.d/xen restart# ist die Migration möglich. 

\section{Live Migration der VM guest1 von pcvirt01 nach pcvirt02}
\subsection{1. Versuch $\Rightarrow$ Bridge fehlt}
Starte VM auf \verb#pcvirt01# und migriere diese von \verb#pcvirt01# nach \verb#pcvirt02#:
\setupVerbatimOut
\begin{verbatim}
xm create guest1.cfg
xm migrate --live guest1 192.168.1.132
\end{verbatim}

Die Migration schlägt fehl, der Maschinenstatus ist verloren. Ursache: Fehlende Brige br-vmonly auf pcvirt02
\\
$\Rightarrow$ Warum wird das nicht in der \emph{Signalling Phase} geprüft und die Migration abgebrochen ohne dass der VM State verloren geht? 

\subsection {2. Versuch $\Rightarrow$ Kernel fehlt}
Starte VM auf \verb#pcvirt01#, erzeuge die fehlende Bridge auf \verb#pcvirt02# und migriere diese von \verb#pcvirt01# nach \verb#pcvirt02#:
\\
Migration schlägt wieder fehl. Ursache: VM verwendet neueren Kernel, der auf Zielsystem \verb#pcvirt02# nicht verfügbar ist.
\\
\\
\large
\textbf{$\Rightarrow$ Warum wird das nicht in der \emph{Signalling Phase} geprüft \dots?}
\normalsize

\subsection {3. Versuch $\Rightarrow$ Falsche Xen Versionen}
Auch nach der Abänderung der Konfiguration der VM auf den älteren Kernel schlägt die Migration wieder fehl. Ursache: VMs lassen sich bei Xen nur auf der gleichen Version live migrieren. Lediglich eine Live Migration von einer Xen Version zur nächst höheren ist möglich \cite{wiki_xen_version}.
\\
\\
\Large
\textbf{$\Rightarrow$ Warum wird das nicht in der \emph{Signalling Phase} geprüft \dots?}
\normalsize

\subsection {4. Versuch $\surd$ }
Der nächste Versuch war schließlich ein Starten der VM auf \verb#pcvirt02# und die Migration auf \verb#pcvirt01#, da hierbei von einer älteren XEN Version auf eine neuere migriert werden müsste.
\setupVerbatimOut
\begin{verbatim} 
xm create guest1.cfg
xm migrate --live guest1 192.168.1.131
\end{verbatim}

Migration erfolgreich, Networkdowntime $<$ als 100ms, da \verb|ping -i 0.1| \verb| -w 0.1 <VM-IP>| keinen Loss anzeigte

\section{Beobachtungen der Migration mit tshark}
Um ein weiteres Verständnis der Migration zu erhalten haben wir nun eine erfolgreiche und eine fehlgeschlagene Migration mit \verb#tshark# aufgezeichnet und analysiert:

\subsection{Erfolgreiche Migration von Xen 4.0 auf 4.1}
Es wird eine TCP Verbindung zwischen beiden Hosts aufgebaut und ein sehr einfaches Protokoll darüber genutzt. Dabei meldet sich \verb#pcvirt02# mit dem \verb|(receive)| command bei \verb#pcvirt01# an. Dieser erlaubt den Empfang über die Rückmeldung \verb|(ready receive)|. Anschließend schickt \verb#pcvirt02# an den \verb#pcvirt1# den \emph{LinuxGuestRecord} mit sämtlichen Settings und States der VM. Darauf schickt \verb#pcvirt02# die RAM Inhalte von \verb#guest1# an \verb#pcvirt01#. Den erfolgreichen Empfang quittiert \verb#pcvirt02# mit einem \verb|(ok)|.

Im Folgenden sieht man einen Ausschnitt des Inhalts der Kommunikation zwischen den Hosts:
\setupVerbatimOut
\begin{verbatim}
# Von >=pcvirt02 nach <=pcvirt01

> (receive)
< (ready receive)
> LinuxGuestRecord
(domain (domid 9) (cpu_weight 256) (cpu_cap 0) (on_crash restart) (uuid 42ed8dbf-c245-2937-f1fc-b2d6d1620aa2) (bootloader_args ) (vcpus 3) (name guest1) (on_poweroff destroy) (on_reboot restart) (cpus (() () ())) (description ) (bootloader ) (maxmem 512) (memory 512) (shadow_memory 0) (vcpu_avail 7) (features ) (on_xend_start ignore) (on_xend_stop ignore) (start_time 1369930035.4) (cpu_time 4.414449199) (online_vcpus 3) (image (linux (kernel /boot/vmlinuz-2.6.32-5-amd64) (ramdisk /boot/initrd.img-2.6.32-5-amd64) (args 'root=/dev/xvda2 ro ') (superpages 0) (tsc_mode 0) (videoram 4) (pci ()) (nomigrate 0) (notes (HV_START_LOW 18446603336221196288) (FEATURES '!writable_page_tables|pae_pgdir_above_4gb') (VIRT_BASE 18446744071562067968) (GUEST_VERSION 2.6) (PADDR_OFFSET 0) (GUEST_OS linux) (HYPERCALL_PAGE 18446744071578882048) (LOADER generic) (SUSPEND_CANCEL 1) (PAE_MODE yes) (ENTRY 18446744071584211456) (XEN_VERSION xen-3.0)))) (status 2) (state -b----) (store_mfn 1190820) (console_mfn 1190819) (device (vif (bridge br-guest) (uuid 7f0b8aa6-f89d-e27c-3ba4-6db424f66858) (script /etc/xen/scripts/vif-bridge) (ip 192.168.10.12) (mac 00:16:3E:95:DE:AD) (backend 0))) (device (vif (bridge br-vmonly) (uuid aef672ce-c13d-2cb2-1b93-040c26ea87ee) (script /etc/xen/scripts/vif-bridge) (ip 192.168.11.12) (mac 00:16:3E:95:BE:EF) (backend 0))) (device (vbd (protocol x86_64-abi) (uuid 1305fde6-95c3-8716-1e73-418fca163709) (bootable 1) (dev xvda2:disk) (uname phy:/dev/etherd/e0.1) (mode w) (backend 0) (VDI ))) (device (vbd (protocol x86_64-abi) (uuid 1e934d97-a69f-f89d-2c52-78e4cee936b6) (bootable 0) (dev xvda1:disk) (uname phy:/dev/etherd/e0.2) (mode w) (backend 0) (VDI ))) (device (console (protocol vt100) (location 2) (uuid 7a60ec21-8fae-9bb0-7223-b3840d2c3c52))) (change_home_server False))
>[viele Daten mainly RAMcopy]
<(ok)
\end{verbatim}

Es ist zu beobachten, dass nach der Übertragung des \emph{GuestRecords} vor der Übertragung der Laufzeitdaten keinerlei Rückkommunikation durch den empfangenden Host stattfindet. Das Protokoll scheint keine Möglichkeit für das aktzeptieren der Migration oder eine Ablehnung an dieser Stelle vorzusehen.

\subsection{Fehlerhafte Migration von Xen 4.1 auf 4.0}

Es läuft wieder wie im vorherigen Abschnitt ab. Nur dass \verb#pcvirt01# und \verb#pcvirt02# vertauscht sind. Anstatt (ok) meldet \verb#pcvirt02# diese Fehlermeldung:
\setupVerbatimOut
\begin{verbatim}
<(err (type "<class 'xen.xend.XendError.XendError'>") (value '/usr/lib/xen-4.0/bin/xc_restore 24 10 1 2 0 0 0 0 failed'))
\end{verbatim}

Dies zeigt, dass nach einer fehlgeschlagenen Migration dies vom empfangenden System gemeldet wird. Warum dies nicht durch den Sendenden Host registriert wird und dort die Maschine trotzdem freigegeben wird ist uns nicht ersichtlich. So findet nämlich leider eine Migration der VM nach \verb#/dev/null# statt. Wenigstens eine Benachrichtigung des Nutzers über die Probleme wäre durchaus angebracht. Hier bleibt zu hoffen, dass XAPI als zukünftiger Ersatz für Xend diese Fehler nicht macht.

\subsection{Gratuitous ARP der VM IP/Mac}

Nach einer erfolgreichen Migration muss, damit die MAC Tabellen der beteiligten Switches/Bridges aktuell sind, Xen noch im Namen der VM einen sog. Gratuitous ARP Request vom Migrations-Ziel absenden dieser sieht so aus:

\setupVerbatimOut
\begin{verbatim}
151619    22.586707000    Xensourc_95:de:ad    Broadcast    ARP    42    Gratuitous ARP for 192.168.1.1 (Request)
\end{verbatim}


\chapter{VLAN für br-vmonly zwischen zwei Hosts}

Damit zwei VMs \verb#guest1# und \verb#guest2#, auch wenn sie nicht beide der selben Hostmaschine laufen, trotzdem über \verb#br-vmlonly# kommunizieren können, muss das LAN in einem VLAN über \verb#br-guest# zwischen den beiden Hosts transportiert werden. Wir wählen die VLAN ID 1234.

\section {Vorarbeiten auf pcvirt02}
Da Gruppe 2 aus dem Praktium ausgestiegen ist, können wir den Host für unsere Zwecke nutzen. Zuerst wird wie in Kapitel \ref{chap:whezzy_ugprade} der Host auf wheezy upgegraded.
Anschließend muss auf pcvirt02 auch das Open vSwitch installiert werden (vgl. Kapitel~\ref{chap:ovs}), die klassiche Bridge entfernt werden und die genutzten Bridges neu erstellt werden:
\setupVerbatim{bash}
\begin{verbatim}
apt-get instal openvswitch-brcompat openvswitch-switch openvswitch-datapath-dkms
modprobe openvswitch-mod
rmmod bridge
ovs-vsctl add-br br-guest
ovs-vsctl add-br br-vmonly
ovs-vsctl add-port br-guest eth0
ifconfig eth0 up
ifconfig br-guest
\end{verbatim}

\verb#/etc/network/interfaces# wurde ebenfalls angepasst:
\setupVerbatimOut
\begin{verbatim}
auto lo eth1
iface lo inet loopback

iface eth1 inet static
    address 10.163.235.132
	netmask 255.255.255.128
	gateway 10.163.235.254

auto br-guest br-vmonly
iface br-guest inet static
        address 192.168.1.132
	netmask 255.255.255.0
	up ifconfig eth0 up

iface br-vmonly inet manual 
	up ifconfig $IFACE up 
\end{verbatim}

\section{VLAN br-vmonly über Switch}
Nun wird die Bridge \verb#br-vmonly# auf beiden Hostsystemen so umgebaut, dass alle Pakete für sie über \verb#br-guest# mit dem VLAN Tag 1234 laufen.

\setupVerbatimOut
\begin{verbatim}
ovs-vsctl del-br br-vmonly
ovs-vsctl add-br br-vmonly br-guest 1234
ifconfig br-vmonly up
\end{verbatim}
Beide VMs wurden nun auf unterschiedlichen Hosts gestartet, allerdings war noch keine Kon\-nek\-tiv\-ität über \verb#br-vmonly# vorhanden.
\\
$\Rightarrow$ Es muss das entsprechende VLAN mit ID 1234 noch auf dem CISCO Switch eingerichtet werden.

\setupVerbatimOut
\begin{verbatim}
# In Configure Modus gehen
switch4cd502# conf t
# Vlan 1234 erstellen
switch4cd502(config)# vlan database 
switch4cd502(config-vlan)# vlan 1234
switch4cd502(config-vlan)# exit
# Für pc-virt[123] interface mit tagged VLAN 1234 einstellen 
switch4cd502(config)# int gi9 
switch4cd502(config-if)# switchport trunk allowed vlan add 1234
switch4cd502(config-if)# exit
switch4cd502(config)# int gi11
switch4cd502(config-if)# switchport trunk allowed vlan add 1234
switch4cd502(config-if)# exit
switch4cd502(config)# int gi13
switch4cd502(config-if)# switchport trunk allowed vlan add 1234
switch4cd502(config-if)# exit
switch4cd502(config)# exit
# Speichern für nächsten Start des Switches
switch4cd502# write memory
\end{verbatim}

Nun ist eine Konnektivität über \verb#br-vmonly# auch Hostübergreifend möglich.

\chapter{DRBD Blockdevicesync}

Mit AoE (vgl. Kapitel \ref{chap:aoe}) kann zwar ein Blockdevice einem anderen Host übers Netz zu Verfügung gestellt werden, wenn man jedoch z.B. \verb#pcvirt01# abschalten muss, kann die VM \verb#guest1# nicht auf \verb#pcvirt02# weiter ausgeführt werden, da der Festplattenspeicher nicht mehr zur Verfügung steht. Aus diesem Grund wird nun das Swap und Root - Blockdevice einer neuen VM \verb#guest3# mittels \emph{Distributed Replicated Block Device} (DRBD) zwischen \verb#pcvirt01# und \verb#pcvirt02# synchronisiert. Somit könnte in oben beschriebenen Fall \verb#pcvirt01# neugestartet werden, ohne dass ein Ausfall von \verb#guest3# drohen würde.

\section{DRBD Installieren/Konfiguieren}
Die Konfiguration in diesem Abschnitt wird auf beiden Hosts (\verb#pcvirt01# und \verb#pcvirt02#) durchgeführt: 
\\
Installation von \emph{DRBD} mittels: 
\setupVerbatimOut
\begin{verbatim}
apt-get install drbd8-utils
\end{verbatim}

Anschließend wird in den globalen Optionen die Sync Geschwindigkeit auf 100Mbit begrenzt:
\\
\verb|/etc/drbd.d/global_config.conf|
\setupVerbatimOut
\begin{verbatim}
global { usage-count no; }
common { syncer { rate 100M; } }
\end{verbatim}

Konfiguration für Root Device:\verb|/etc/drbd.d/guest1-root.res|
\setupVerbatimOut
\begin{verbatim}
resource guest1-root {
        protocol C;
        net {
                allow-two-primaries;
                cram-hmac-alg sha1;
                shared-secret "d9a8sdtgejho3";
                after-sb-0pri discard-younger-primary; #discard-zero-changes;
                after-sb-1pri discard-secondary;
                after-sb-2pri call-pri-lost-after-sb;
        }
        on pcvirt01 {
                device /dev/drbd1;
                disk /dev/storage/drbd-guest1-root;
                address 192.168.1.131:7789;
                meta-disk internal;
        }
        on pcvirt02 {
                device /dev/drbd1;
                disk /dev/storage/drbd-guest1-root;
                address 192.168.1.132:7789;
                meta-disk internal;
        }
}
\end{verbatim}

Konfiguration für Swap Device: \verb|/etc/drbd.d/guest1-swap.res|
\setupVerbatimOut
\begin{verbatim}
resource guest1-swap {
        protocol C;
        net {
                allow-two-primaries;
                cram-hmac-alg sha1;
                shared-secret "d9a8sdtgej124";
                after-sb-0pri discard-younger-primary; #discard-zero-changes;
                after-sb-1pri discard-secondary;
                after-sb-2pri call-pri-lost-after-sb;
        }
        on pcvirt01 {
                device /dev/drbd2;
                disk /dev/storage/drbd-guest1-swap;
                address 192.168.1.131:7790;
                meta-disk internal;
        }
        on pcvirt02 {
                device /dev/drbd2;
                disk /dev/storage/drbd-guest1-swap;
                address 192.168.1.132:7790;
                meta-disk internal;
        }
}
\end{verbatim}


\section{DRBD intialisieren}

Nun wird \emph{DRBD} initialisiert und das erste mal gesynct. Davor muss noch auf \verb#pcvirt02# eine LVM Volume Group eingerichtet werden:

\setupVerbatimOut
\begin{verbatim}
vgcreate storage /dev/sdb1 
\end{verbatim}

Anschließend auf beiden Hosts die \emph{Backend-Devices} für DRBD erstellen:
\setupVerbatimOut
\begin{verbatim}
lvcreate -L6G -n drbd-guest1-root storage
lvcreate -L256M -n drbd-guest1-swap storage
\end{verbatim}

Starte DRBD und formatiere die Backend-Devices (auf beiden Hosts):

\setupVerbatimOut
\begin{verbatim}
service drbdrstart
drbdadm create-md guest1-swap
drbdadm create-md guest1-root
\end{verbatim}


Nun wird auf einem der Hosts der intiale Sync von den beiden DRBD Devices \verb#/dev/drbd1# und \verb#/dev/drbd2# durchgeführt:

\setupVerbatimOut
\begin{verbatim}
drbdadm -- --overwrite-data-of-peer primary all
\end{verbatim}
In \verb#/proc/drbd# kann man den Sync beobachten:

\setupVerbatimOut
\begin{verbatim}
version: 8.3.11 (api:88/proto:86-96)
srcversion: 41C52C8CD882E47FB5AF767 

 1: cs:SyncSource ro:Secondary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:747264 nr:0 dw:0 dr:755456 al:0 bm:44 lo:2 pe:158 ua:128 ap:0 ep:1 wo:f oos:5452576
        [=>..................] sync'ed: 12.0\% (5324/6044)Mfinish: 0:00:59 speed: 92,144 (92,144) K/sec
 2: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
    ns:160728 nr:0 dw:0 dr:160728 al:0 bm:10 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
\end{verbatim}
Hier sieht man dass Nr. 1 noch synchronisiert wird und der  Forschritt bei 12,0\% liegt und  Nr. 2 bereits vollständig synchronisert wurde, was man an \verb#UpToDate/UpToDate# sieht.
Wenig später betrachtet ist die Ausgabe wie folgt und die Synchronisation daher abgeschlossen:

\setupVerbatimOut
\begin{verbatim}
version: 8.3.11 (api:88/proto:86-96)
srcversion: 41C52C8CD882E47FB5AF767 

 1: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
    ns:6189728 nr:0 dw:0 dr:6189728 al:0 bm:378 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
 2: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
    ns:160728 nr:0 dw:0 dr:160728 al:0 bm:10 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
\end{verbatim}

Zu guter Letzt müssen auf beiden Hosts noch die Devices in den \emph{Primary State} versetzt werden, nur im \emph{Primary} Modus stehen die Devices zum lesen und schreiben zur Verfügung:
\setupVerbatimOut
\begin{verbatim}
drbdadm primary all
\end{verbatim}

\section{Guest 3 installieren}
Nun wird ein neuer Xen Guest auf den DRBD Devices installiert und gestartet:
\setupVerbatimOut
\begin{verbatim}
xen-create-image --ip 192.168.1.3 --bridge=br-guest  --swap-dev=/dev/drbd2 --password=123 --image-dev=/dev/drbd1 --hostname=guest3 --vcpus=2 --dist wheezy
xm create /etc/xen/guest3.cfg
\end{verbatim}

\verb|guest3| kann nun frei zwischen \verb#pcvirt01# und \verb#pcvirt02# verschoben werden, dabei ist nur der Rechner nötig auf dem die VM läuft der andere kann in der Zeit offline gehen, neugestartet oder gewartet werden.
